{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudstorage in /opt/conda/lib/python3.7/site-packages (0.10.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from cloudstorage) (2.8.1)\n",
      "Requirement already satisfied: inflection>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from cloudstorage) (0.5.0)\n",
      "Requirement already satisfied: python-magic>=0.4.15 in /opt/conda/lib/python3.7/site-packages (from cloudstorage) (0.4.18)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->cloudstorage) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install cloudstorage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (20.3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2\n",
      "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2 MB 2.5 kB/s  eta 0:00:01    |█████▉                          | 94.6 MB 84.4 MB/s eta 0:00:05     |██████▍                         | 102.9 MB 84.4 MB/s eta 0:00:05\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.12.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (2.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.33.2)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (0.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.18.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.14.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (0.1.8)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (3.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (0.34.2)\n",
      "Requirement already satisfied: scipy==1.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.4.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (3.11.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (0.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.14.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (0.34.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.14.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.14.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.14.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.18.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.14.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.18.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.14.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.18.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow==2.2) (45.2.0.post20200209)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.18.1)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 62.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (0.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.14.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.11.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.18.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (3.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow==2.2) (45.2.0.post20200209)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.33.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (0.34.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.2.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.1.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow==2.2) (45.2.0.post20200209)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.2) (1.14.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.2.7)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.11.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow==2.2) (45.2.0.post20200209)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (1.25.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.9)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (2.23.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (3.0.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2) (0.4.8)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 59.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.3.0\n",
      "    Uninstalling tensorflow-estimator-2.3.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.4.0\n",
      "    Uninstalling tensorboard-2.4.0:\n",
      "      Successfully uninstalled tensorboard-2.4.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.3.1\n",
      "    Uninstalling tensorflow-2.3.1:\n",
      "      Successfully uninstalled tensorflow-2.3.1\n",
      "Successfully installed tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow==2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.4'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.1\n",
      "  Downloading pandas-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5 MB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1) (1.18.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.1) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.5\n",
      "    Uninstalling pandas-1.1.5:\n",
      "      Successfully uninstalled pandas-1.1.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-data-validation 0.21.5 requires joblib<0.15,>=0.12, but you have joblib 0.15.1 which is incompatible.\n",
      "tensorflow-data-validation 0.21.5 requires pandas<1,>=0.24, but you have pandas 1.1.0 which is incompatible.\n",
      "tensorflow-data-validation 0.21.5 requires scikit-learn<0.22,>=0.18, but you have scikit-learn 0.23.1 which is incompatible.\u001b[0m\n",
      "Successfully installed pandas-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas==1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0-dlenv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from keras) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras) (1.18.4)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras) (1.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as tfk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tqdm\n",
    "#!pip install htop\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import joblib\n",
    "import sys\n",
    "\n",
    "import scipy.signal as sp\n",
    "from scipy.ndimage import median_filter\n",
    "from scipy.ndimage import uniform_filter\n",
    "import time\n",
    "import traceback\n",
    "from joblib.externals.loky import set_loky_pickler\n",
    "from joblib import parallel_backend\n",
    "from joblib import Parallel, delayed\n",
    "import scipy.stats as sps\n",
    "from joblib import wrap_non_picklable_objects\n",
    "\n",
    "\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#clf = DecisionTreeClassifier(random_state=0)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from joblib.externals.loky import set_loky_pickler\n",
    "from joblib import parallel_backend\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import wrap_non_picklable_objects\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from scipy.stats import mode\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm_notebook\n",
    "import joblib\n",
    "import sys\n",
    "import warnings\n",
    "import random\n",
    "from skimage.transform import resize\n",
    "\n",
    "import scipy.signal as sp\n",
    "from scipy.ndimage import median_filter\n",
    "from scipy.ndimage import uniform_filter\n",
    "import time\n",
    "import traceback\n",
    "from joblib.externals.loky import set_loky_pickler\n",
    "from joblib import parallel_backend\n",
    "from joblib import Parallel, delayed\n",
    "import scipy.stats as sps\n",
    "from joblib import wrap_non_picklable_objects\n",
    "from itertools import permutations\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil cp -r gs://instance-transfers/segmentation_scanconvert_15_256_400.json  /home/jupyter/AdisStuff\n",
    "gsutil cp -r gs://instance-transfers/segmentation_scanconvert_15_256_400.h5  /home/jupyter/AdisStuff\n",
    "''' copying mask model weights'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file3 = open('segmentation_scanconvert_15_256_400.json', 'r')\n",
    "loaded_model_json3 = json_file3.read()\n",
    "json_file3.close()\n",
    "loaded_model3 = model_from_json(loaded_model_json3)\n",
    "# load weights into new model\n",
    "loaded_model3.load_weights(\"segmentation_scanconvert_15_256_400.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best_all_data_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudstorage as gcs\n",
    "from google.cloud import storage\n",
    "storage_client = storage.Client()\n",
    "\n",
    "bucket_pull_name =\"liver-data-48x61-alldata-train\"#\"liver-data-train-5\"#\"liver-data-train-4\"#\"liver-data-24x122-alldata-train\"# \"liver-data-train-val\"\n",
    "'''sanse: select your own data bucket'''\n",
    "bucket_pull = storage_client.get_bucket(bucket_pull_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video(data_type):\n",
    "    ''' sanse: get filenames of each video, we do not need to know which patient it is at this point'''\n",
    "    length = len(data_type) + 3\n",
    "    # get all patient IDS - use the rf VIDEO folder for simplicity\n",
    "    # get the blob iterator objects\n",
    "    blob = bucket_pull.list_blobs()\n",
    "    # get relevant file names from blob objects\n",
    "    fileList1 = [file.name for file in blob if ( file.name.endswith((data_type+'.npy')) and file.name.startswith((data_type+'_v/')) ) ]\n",
    "    fileList1 = np.asarray(fileList1)\n",
    "    size = np.size(fileList1)\n",
    "    print(size)\n",
    "#     ID_list1 = np.copy(fileList1)\n",
    "#     for ii in range(0, size):\n",
    "#         ID_list1[ii] = fileList1[ii][length:length+16]\n",
    "#     # avoid duplicates\n",
    "#     ID_list1 = np.unique(ID_list1)\n",
    "#     # get all video names\n",
    "#     size = np.shape(fileList1)[0]\n",
    "#     fileList = np.copy(fileList1)\n",
    "#     caseID = np.copy(fileList)\n",
    "#     videID = np.copy(fileList)\n",
    "#     bothID = np.copy(fileList)\n",
    "#     for ii in range(0, size):\n",
    "#         fileList[ii] = fileList1[ii][length:]\n",
    "#         split = fileList[ii].split('_')\n",
    "#         caseID[ii] = split[0]\n",
    "#         videID[ii] = split[1]\n",
    "#         bothID[ii] = split[0] + '_' + split[1]\n",
    "#     caseID = np.unique(caseID)\n",
    "#     videID = np.unique(videID)\n",
    "    bothID2 = np.unique(fileList1)\n",
    "    print(np.size(bothID2))\n",
    "    return  bothID2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf2bmode(rf):\n",
    "    ''' sanse: Same as in DSP_, different from Kfold_, good for segmentation'''\n",
    "    import numpy as np\n",
    "    from scipy.signal import hilbert,decimate,resample\n",
    "\n",
    "\n",
    "    # decimation factor use to create Env from RF\n",
    "    decimationFactor = 1\n",
    "\n",
    "    # make a compression table\n",
    "    alpha = 0.55# sqrt compression, change for a different compression table\n",
    "    denom = np.exp(alpha * np.log(65535)) / 255.0\n",
    "    CompressionTable = np.exp(alpha * np.log(np.arange(0,65536))) / denom \n",
    "    # CompressionTable = np.exp(alpha * np.log(65535)) / denom\n",
    "    \n",
    "    # calculate envelope and log compress\n",
    "    Env = 1 + np.fix(np.abs(hilbert(rf, axis= 0)))\n",
    "    Env = CompressionTable[np.array(Env, dtype=int)]\n",
    "\n",
    "    EnvDec =resample(Env ,int(len(Env[ : , 0])/decimationFactor))\n",
    "    \n",
    "    EnvDec[(EnvDec>255).nonzero()]=255\n",
    "#EnvDec[(EnvDec>255).nonzero()]=255\n",
    "    return EnvDec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"BackScatter Coefficient Rectangular Window\"))-0\\n \"ESD Rectangular Window\"))-1\\n     \"EAC Rectangular Window\"))-2\\n \"Spectral Slope Rectangular Window\"))-3\\n    \"Spectral Intercept Rectangular Window\"))-4\\n  \"MidBand Fit Rectangular Window\"))-5\\n \"Attenuation Coefficient Hamming Window\"))-6\\n\"BackScatter Coefficient Hamming Window\"))-7\\n \"ESD Hamming Window\"))-8\\n \"EAC Hamming Window\"))-9\\n \"Spectral Slope Hamming Window\"))-10\\n\"Spectral Intercept Hamming Window\"))-11\\n \"MidBand Fit Hamming Window\"))-12\\n      \"Nakagami Spread\"))-13\\n \"Nakagami Shape\"-14\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\"BackScatter Coefficient Rectangular Window\"))-0\n",
    " \"ESD Rectangular Window\"))-1\n",
    "     \"EAC Rectangular Window\"))-2\n",
    " \"Spectral Slope Rectangular Window\"))-3\n",
    "    \"Spectral Intercept Rectangular Window\"))-4\n",
    "  \"MidBand Fit Rectangular Window\"))-5\n",
    " \"Attenuation Coefficient Hamming Window\"))-6\n",
    "\"BackScatter Coefficient Hamming Window\"))-7\n",
    " \"ESD Hamming Window\"))-8\n",
    " \"EAC Hamming Window\"))-9\n",
    " \"Spectral Slope Hamming Window\"))-10\n",
    "\"Spectral Intercept Hamming Window\"))-11\n",
    " \"MidBand Fit Hamming Window\"))-12\n",
    "      \"Nakagami Spread\"))-13\n",
    " \"Nakagami Shape\"-14\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get videos and frame numbers for each case\n",
    "info_list = []\n",
    "\n",
    "for ii in range(0, np.shape(caseID)[0]):\n",
    "    # set up info list\n",
    "    info_list.append( [] )\n",
    "    \n",
    "    # get ID\n",
    "    info_list[ii].append(caseID[ii])\n",
    "    \n",
    "    # get video list\n",
    "    vidlist = [ jj for jj in bothID2 if caseID[ii] in jj ]\n",
    "    vidlist =  [ jj[17:] for jj in vidlist ]\n",
    "    info_list[ii].append(vidlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = {}\n",
    "case_dict = {}\n",
    "for ii in range(0, len(info_list)):\n",
    "    \n",
    "    case_dict = {'videos': info_list[ii][1] }\n",
    "    \n",
    "    info_dict[info_list[ii][0]] = case_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load video directly from _v/ folder\n",
    "def load_rf_video( ID ):   \n",
    "    filename = ID#'rf_v/' + ID + '_' + vid + '_rf.npy' \n",
    "    temp = storage.blob.Blob(filename,bucket_pull)\n",
    "    content = temp.download_as_string()\n",
    "    video = np.load(BytesIO(content))\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.lib.io import file_io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_masks_1_frame(rf_frame,iminf):\n",
    "    '''sanse: model is only trained in part of the bmode image'''\n",
    "    #res = np.reshape(cv2.resize(B_mode[86:886,139:1339], dsize=(0,256, 400), interpolation=cv2.INTER_CUBIC),(1, 256, 400,1))\n",
    "    bmode_reshaped = np.reshape(scanconvert(rf2bmode(rf_frame),info=iminf),(1,973,1478,1))\n",
    "    bmode_resized = np.zeros((1,256,400,1))\n",
    "    bmode_resized[0,:,:,:] = resize(bmode_reshaped[0,86:886,139:1339,:], bmode_resized.shape[1:], anti_aliasing=True)\n",
    "    return bmode_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(array, xx, yy):\n",
    "\n",
    "    h = array.shape[0]\n",
    "    w = array.shape[1]\n",
    "\n",
    "    a = (xx - h) // 2\n",
    "    aa = xx - a - h\n",
    "\n",
    "    b = (yy - w) // 2\n",
    "    bb = yy - b - w\n",
    "\n",
    "    return np.pad(array, pad_width=((a, aa), (b, bb)), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_post_postprocessing_r(predicted):\n",
    "    '''sanse: predicted mask needs to be resized and gaps need to be filled (check)'''\n",
    "    #predicted = mean_5(predicted[0,:,:,0])\n",
    "    #thresh,sample_mask_mean5_otsu = cv2.threshold(((sample_mask_mean5*255).astype(\"uint8\")),0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    sample_mask_mean5_otsu = np.where(predicted[0,:,:,0]>0.5,1,0)\n",
    "    predicted1 = np.array(cv2.resize(sample_mask_mean5_otsu.astype(\"uint8\"), dsize=(1200, 800), interpolation=cv2.INTER_CUBIC))\n",
    "    predicted1 = padding(predicted1,973,1478)\n",
    "    \n",
    "#     predicted2 = np.array(cv2.resize(predicted[0,:,:,0].astype(\"uint8\"), dsize=(1200, 800), interpolation=cv2.INTER_CUBIC))\n",
    "#     predicted2 = padding(predicted2,973,1478)\n",
    "    return predicted1#,predicted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def maping_back(predicted_mask,iminf):\n",
    "    '''sanse: we need scan converted image for segmentation to work properly, so we need to reshape to rf coord system'''\n",
    "    rf = np.empty((2928,192))\n",
    "    LABELLEDIMAGE =predicted_mask#mask_post_postprocessing_r(predicted_2)# For pkl, use that, otherwise may need: BytesIO(content))\n",
    "    #Load RF Image\n",
    "\n",
    "    #Creating the Map. Arguabbly unnecsary to recreate it each time, but/we\n",
    "    armap2=np.arange(0,np.size(rf))\n",
    "    armap2=armap2.reshape((np.shape(rf)[0],-1))\n",
    "     #need for scanconvert\n",
    "    MAPIMAGE=scanconvert(armap2,iminf)\n",
    "    BWMASK=LABELLEDIMAGE\n",
    "    indi=MAPIMAGE[(BWMASK).nonzero()]\n",
    "    indi2=np.unique(indi)\n",
    "    #p.shape(indi2)\n",
    "\n",
    "    a=np.zeros(np.size(rf))\n",
    "    a[indi2.astype(int)]=1\n",
    "    rfmask=np.reshape(a,(np.shape(rf))) #Final Mask\n",
    "    rfmask = scipy.ndimage.uniform_filter(rfmask, size=(80,1))\n",
    "    rfmask[rfmask!=0]=1\n",
    "    return rfmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(rf_frame,model,iminf):\n",
    "    bmode_resized = resize_masks_1_frame(rf_frame,iminf) '''crop relevant area and scale to network'''\n",
    "    predicted = model.predict(bmode_resized)\n",
    "    #predicted1,predicted2 = mask_post_postprocessing_r(predicted)\n",
    "    predicted1 = mask_post_postprocessing_r(predicted) ''' fill holes'''\n",
    "    predicted1 = maping_back(predicted1,iminf) ''' get back to rf coordinate system'''\n",
    "    return predicted[0,:,:,0],predicted1#,rf2bmode(rf_frame)#,predicted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "iminf=joblib.load('Iminf_.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanconvert(Iin=None, info=None):#Apitch, Lpitch, Radius, PixelsPerMM)\n",
    "    '''sanse: it is fine, Adi has a more efficient version (align with him for speed optimization)'''\n",
    "    import numpy as np\n",
    "    if (info[\"Radius\"] == 0):    # A linear array probe\n",
    "        print(\"Linear Array\")\n",
    "        AxialExtent = info[\"Apitch\"] * np.size(Iin, 0)\n",
    "        LateralExtent = info[\"Lpitch\"] * np.size(Iin, 1)\n",
    "        AxialSize = AxialExtent * 1000 * info[\"PixelsPerMM\"]\n",
    "        LateralSize = LateralExtent * 1000 * info[\"PixelsPerMM\"]\n",
    "        Iout = np.resize(Iin, (LateralSize, AxialSize))    # cubic interpolation\n",
    "    else:    # A curved array probe    \n",
    "        t = ((np.arange(1,Iin.shape[1],dtype=int)) - Iin.shape[1]/ 2) * info[\"Lpitch\"] / info[\"Radius\"]\n",
    "        r = info[\"Radius\"] + (np.arange(1,Iin.shape[0])) * info[\"Apitch\"]\n",
    "        [t, r] = np.meshgrid(t, r)\n",
    "        x = np.multiply(r, np.cos(t))\n",
    "        y = np.multiply(r, np.sin(t))\n",
    "    \n",
    "        divides=1e-3 / info[\"PixelsPerMM\"]\n",
    "        xMin=np.min(x)\n",
    "        xMax=np.max(x)\n",
    "        #print(\"X Min : %s , X Max %s\" % (xMin,xMax))\n",
    "        #print(\"Divides : %s \" % divides)\n",
    "        xreg =  np.arange(np.min(x) , np.max(x) ,divides)\n",
    "        yreg =  np.arange(np.min(y) , np.max(y) ,divides)\n",
    "        [yreg, xreg] = np.meshgrid(yreg, xreg) \n",
    "        Iout = np.zeros(xreg.shape)\n",
    "        xCntr = np.arange(0,xreg.shape[1])\n",
    "        Cntry = np.arange(0,yreg.shape[0])\n",
    "        theta = np.array([cart4pol(xreg[yCntr, xCntr], yreg[yCntr, xCntr]) for yCntr in Cntry])\n",
    "        rho = np.array([cart2pol(xreg[yCntr, xCntr], yreg[yCntr, xCntr]) for yCntr in Cntry])\n",
    "        #for xCntr in tqdm(np.arange(0,xreg.shape[1])):\n",
    "            #for yCntr in np.arange(0,yreg.shape[0]):\n",
    "                #[theta, rho] = cart2pol(xreg[yCntr, xCntr], yreg[yCntr, xCntr])\n",
    "                #print(\"theta : %s , rho %s\" % (theta,rho))\n",
    "        indt = np.floor(theta / (info[\"Lpitch\"] / info[\"Radius\"]) + Iin.shape[1] / 2) \n",
    "        indr = np.floor((rho - info[\"Radius\"]) / info[\"Apitch\"]) \n",
    "\n",
    "        [j,i]=((indt>=0)&(indt <= t.shape[1])& (indr>= 0) &(indr <= r.shape[0])).nonzero()\n",
    "        #print(np.size(j))\n",
    "        j=j.astype(int)\n",
    "        i=i.astype(int)\n",
    "        Iout[j, i] = Iin[indr[j,i].astype(int), indt[j,i].astype(int)]\n",
    "        #for i in xCntr:\n",
    "            \n",
    "         #   for j in Cntry:\n",
    "                \n",
    "          #      if indt[j,i] >= 0 and (indt[j,i] <= t.shape[1]and (indr[j,i] >= 0 and indr[j,i] <= r.shape[0])):\n",
    "                    \n",
    "           #         Iout[int(Cntry[j]), int(xCntr[i])] = Iin[int(indr[j,i]), int(indt[j,i])]\n",
    "    #print(np.shape(Iout))\n",
    "    return Iout\n",
    "\n",
    "def cart2pol(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    return rho\n",
    "def cart4pol(x, y):\n",
    "    phi = np.arctan2(y, x)\n",
    "    return phi\n",
    "\n",
    "\n",
    "def pol2cart(rho, phi):\n",
    "    x = rho * np.cos(phi)\n",
    "    y = rho * np.sin(phi)\n",
    "    return(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Load_n_save_cnns_2(files,model,iminf):\n",
    "    '''sanse: saves three different versions of the mask'''\n",
    "    windtoframe=[]\n",
    "    datavidtopat = []\n",
    "    datavid = []\n",
    "    datalabvid = []\n",
    "    dataframetovid = []\n",
    "    sergg=0\n",
    "    dataframe = []\n",
    "    datawind = []\n",
    "    datalabframe = []\n",
    "    qusweights=[]\n",
    "    rfweights=[]\n",
    "    quantweights=[]    \n",
    "    bucket_push_name=\"liver-data-48x61-alldata-train-n-3\"#\"\"liver-data-train-4-n\"\n",
    "    '''sanse: change to own bucket'''\n",
    "\n",
    "    for m in tqdm_notebook(range(0,np.shape(files)[0])): \n",
    "        vidfn=load_rf_video(files[m])\n",
    "        Rf_list2=vidfn\n",
    "        \n",
    "        \n",
    "        #Bmode = np.array([(rf2bmode(Rf_list2[q,:,:])) for q in range(0,np.shape(vidfn)[0])])#rf2bmode(ModeIM)#\n",
    "        '''\n",
    "        try:\n",
    "            Bmode = rf2bmode(Rf_list2)\n",
    "        except:\n",
    "            datavidtopat[m]=datavidtopat[m]-1\n",
    "            continue\n",
    "        '''\n",
    "        #bmode=np.reshape(Bmode,(np.shape(Bmode)[0],np.shape(Bmode)[1],np.shape(Bmode)[2],1))\n",
    "\n",
    "\n",
    "        #bmode=bmode/211#np.max(bmode)#this line for normalization\n",
    "        #print(np.shape(Bmode))\n",
    "        #print(np.shape(bmode))\n",
    "        #predicted_mask = loaded_model.predict(bmode)\n",
    "        #predicted_mask_bad=np.array([(loaded_model_2.predict(bmode[q,:,:,:].reshape((1,2928,192,1)))) for q in range(0,np.shape(Rf_list2)[0])])\n",
    "        preds=np.array([(run_all(Rf_list2[q,:,:],model,iminf)) for q in range(0,np.shape(vidfn)[0])])#\n",
    "        \n",
    "        pred1=np.array([(preds[q,0]) for q in range(0,np.shape(preds)[0])])#rf2bmode(ModeIM)#\n",
    "        pred2=np.array([(preds[q,1]) for q in range(0,np.shape(preds)[0])])#rf2bmode(ModeIM)#\n",
    "        #pred3=np.array([(preds[q,2]) for q in range(0,np.shape(preds)[0])])#rf2bmode(ModeIM)#\n",
    "        #pred1,pred2,pred3=np.array([(run_all(Rf_list2[q,:,:],model,iminf)) for q in range(0,np.shape(vidfn)[0])])#\n",
    "        #pred1=preds[:,0]\n",
    "        #pred2=preds[:,1]\n",
    "        #pred3=preds[:,2]\n",
    "#         print(np.shape(pred1))\n",
    "#         print(np.shape(pred2))\n",
    "#         print(np.shape(vidfn))\n",
    "        pred4=copy.deepcopy(pred2)\n",
    "        pred4[(pred4==0).nonzero()]=np.nan\n",
    "        pred4=pred4*vidfn\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #predicted_mask=np.array([(loaded_model.predict(bmode[q,:,:,:].reshape((1,2928,192,1)))) for q in range(0,np.shape(Rf_list2)[0])])\n",
    "        \n",
    "            \n",
    "        #predicted_mask_good=np.array([(loaded_model_b.predict(bmode[q,:,:,:].reshape((1,2928,192,1)))) for q in range(0,np.shape(Rf_list2)[0])])\n",
    "        \n",
    "        np.save( file_io.FileIO( ( 'gs://' + bucket_push_name + '/' + ( 'origmsk'+'/'+files[m][2:-6]+'msk.npy') ), 'w') , pred1 )\n",
    "        '''sanse: origmsk -> small mask after scan-converted mask (as filtering info of liver %) '''\n",
    "        np.save( file_io.FileIO( ( 'gs://' + bucket_push_name + '/' + ( 'bigmask'+'/'+files[m][2:-6]+'msk.npy') ), 'w') , pred2 )\n",
    "        '''sanse: resize one into full image size (currently used as mask)'''\n",
    "        np.save( file_io.FileIO( ( 'gs://' + bucket_push_name + '/' + ( 'mskdrf'+'/'+files[m][2:-6]+'msk.npy') ), 'w') , pred4 )\n",
    "        '''sanse: rf postmasking (we don't use right now straightway but thought for rf extension)'''\n",
    "\n",
    "#         print(np.shape(predicted_mask_bad))\n",
    "#         print(np.shape(predicted_mask))\n",
    "#         print(np.shape(predicted_mask_good))   \n",
    "        \n",
    "        #del predicted_mask_good\n",
    "        del vidfn\n",
    "        del preds\n",
    "        del pred4\n",
    "        del Rf_list2\n",
    "        del pred2\n",
    "        del pred1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(np.shape(predicted_mask_bad))\n",
    "#         print(np.shape(predicted_mask))\n",
    "#         print(np.shape(predicted_mask_good))   \n",
    "                \n",
    "    return [m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e1fd4e84b440429a16e5399e26854e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9772.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "Load_n_save_cnns_2(bothID2,loaded_model3,iminf)\n",
    "'''sanse: RUNME'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_n_print_cnns(files, model, iminf):\n",
    "    windtoframe=[]\n",
    "    datavidtopat = []\n",
    "    datavid = []\n",
    "    datalabvid = []\n",
    "    dataframetovid = []\n",
    "    sergg=0\n",
    "    dataframe = []\n",
    "    datawind = []\n",
    "    datalabframe = []\n",
    "    qusweights=[]\n",
    "    rfweights=[]\n",
    "    quantweights=[]    \n",
    "    #bucket_push_name=\"liver-data-train-5\"#\"\"liver-data-train-4-n\"\n",
    "    \n",
    "    for m in tqdm_notebook(range(2006,2007)):#np.shape(files)[0])): \n",
    "        vidfn=load_rf_video(files[m])\n",
    "        Rf_list2=vidfn\n",
    "        \n",
    "        \n",
    "        Bmode = np.array([(rf2bmode(Rf_list2[q,:,:])) for q in range(0,np.shape(vidfn)[0])])#rf2bmode(ModeIM)#\n",
    "        '''\n",
    "        try:\n",
    "            Bmode = rf2bmode(Rf_list2)\n",
    "        except:\n",
    "            datavidtopat[m]=datavidtopat[m]-1\n",
    "            continue\n",
    "        '''\n",
    "        bmode=np.reshape(Bmode,(np.shape(Bmode)[0],np.shape(Bmode)[1],np.shape(Bmode)[2],1))\n",
    "        preds=np.array([(run_all(Rf_list2[q,:,:],model,iminf)) for q in range(0,np.shape(vidfn)[0])])#\n",
    "        print(np.shape(preds))\n",
    "        pred1=np.array([(preds[q,0]) for q in range(0,np.shape(preds)[0])])#rf2bmode(ModeIM)#\n",
    "        pred2=np.array([(preds[q,1]) for q in range(0,np.shape(preds)[0])])#rf2bmode(ModeIM)#\n",
    "        pred3=np.array([(preds[q,2]) for q in range(0,np.shape(preds)[0])])#rf2bmode(ModeIM)#\n",
    "        #pred1,pred2,pred3=np.array([(run_all(Rf_list2[q,:,:],model,iminf)) for q in range(0,np.shape(vidfn)[0])])#\n",
    "        #pred1=preds[:,0]\n",
    "        #pred2=preds[:,1]\n",
    "        #pred3=preds[:,2]\n",
    "        print(np.shape(pred1))\n",
    "        print(np.shape(pred2))\n",
    "        print(np.shape(vidfn))\n",
    "        pred4=copy.deepcopy(pred2)\n",
    "        pred4[(pred4==0).nonzero()]=np.nan\n",
    "        pred4=pred4*vidfn\n",
    "        #print(np.shape(pred3))\n",
    "        for a in range(0,np.shape(vidfn)[0]):#])#): #(len(rf)):\n",
    "#             frame = a\n",
    "#             #new bmode prediction\n",
    "#             bmode = scanconvert(Iin=image_improvement(rf[frame], 3.5e6, 20000000, 3, rf2bmode(rf[frame])), info=imgInfo)\n",
    "#             bmode_reshaped = np.reshape(bmode,(1,973,1478,1))\n",
    "#             bmode_resized = np.zeros((1,224,224,1))\n",
    "#             bmode_resized[0,:,:,:] = resize(bmode_reshaped[0,:,:,:], bmode_resized.shape[1:], anti_aliasing=True)\n",
    "#             predicted1 = loaded_model2.predict(bmode_resized)\n",
    "#             #new bmode prediction - raulz\n",
    "#             bmode_raul = resize_masks_1_frame(scanconvert(rf2bmode(rf[frame]), info=imgInfo))\n",
    "#             predicted_2 = loaded_model3.predict(bmode_raul)\n",
    "#             #old bmode prediction (just printing mask)\n",
    "#             bmode_2 = mask[a,0,:,:,0]\n",
    "#             predicted_3 = scanconvert(Iin=bmode_2, info=imgInfo)\n",
    "            #plot\n",
    "            f, (ax1, ax2, ax4,ax5,ax3) = plt.subplots(1, 5, figsize = (20,20))\n",
    "            ax1.imshow(Bmode[a], extent=[0,200,0,200], cmap = 'gray')\n",
    "            ax2.imshow(pred1[a], cmap = 'gray')#extent=[0,200,0,200], cmap = 'gray')\n",
    "            ax2.title.set_text('New Model - Miriam')\n",
    "            ax3.imshow(Bmode[a]*pred2[a], extent=[0,200,0,200],cmap = 'gray')\n",
    "            ax5.title.set_text('New Model - Raul - t')\n",
    "            ax5.imshow(pred2[a], extent=[0,200,0,200],cmap = 'gray')\n",
    "            ax3.title.set_text('New Model - Raul')\n",
    "            ax4.imshow(pred4[a], extent=[0,200,0,200], cmap = 'gray')\n",
    "            ax4.title.set_text('New model - Miriam - t')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        #bmode=bmode/211#np.max(bmode)#this line for normalization\n",
    "        #print(np.shape(Bmode))\n",
    "        #print(np.shape(bmode))\n",
    "        #predicted_mask = loaded_model.predict(bmode)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         predicted_mask_bad=np.array([(loaded_model_2.predict(bmode[q,:,:,:].reshape((1,2928,192,1)))) for q in range(0,np.shape(Rf_list2)[0])])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "#         predicted_mask=np.array([(loaded_model.predict(bmode[q,:,:,:].reshape((1,2928,192,1)))) for q in range(0,np.shape(Rf_list2)[0])])\n",
    "        \n",
    "            \n",
    "#         predicted_mask_good=np.array([(loaded_model_b.predict(bmode[q,:,:,:].reshape((1,2928,192,1)))) for q in range(0,np.shape(Rf_list2)[0])])\n",
    "        \n",
    "#         np.save( file_io.FileIO( ( 'gs://' + bucket_push_name + '/' + ( 'oldmodel'+files[m][2:-6]+'msk.npy') ), 'w') , predicted_mask )\n",
    "        \n",
    "#         np.save( file_io.FileIO( ( 'gs://' + bucket_push_name + '/' + ( 'newmodel'+files[m][2:-6]+'msk.npy') ), 'w') , predicted_mask_good )\n",
    "        \n",
    "#         np.save( file_io.FileIO( ( 'gs://' + bucket_push_name + '/' + ( 'prctmodel'+files[m][2:-6]+'msk.npy') ), 'w') , predicted_mask_bad )\n",
    "        \n",
    "#         print(np.shape(predicted_mask_bad))\n",
    "#         print(np.shape(predicted_mask))\n",
    "#         print(np.shape(predicted_mask_good))   \n",
    "        \n",
    "#         del predicted_mask_good\n",
    "#         del predicted_mask\n",
    "#         del predicted_mask_bad\n",
    "#         del bmode\n",
    "#         del Rf_list2\n",
    "#         del Bmode\n",
    "#         del vidfn\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(np.shape(predicted_mask_bad))\n",
    "#         print(np.shape(predicted_mask))\n",
    "#         print(np.shape(predicted_mask_good))   \n",
    "                \n",
    "    return pred4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9772,)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(bothID2)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
