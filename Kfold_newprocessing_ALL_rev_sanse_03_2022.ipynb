{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudstorage in /opt/conda/lib/python3.7/site-packages (0.10.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from cloudstorage) (2.8.1)\n",
      "Requirement already satisfied: python-magic>=0.4.15 in /opt/conda/lib/python3.7/site-packages (from cloudstorage) (0.4.18)\n",
      "Requirement already satisfied: inflection>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from cloudstorage) (0.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->cloudstorage) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install cloudstorage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as tfk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tqdm\n",
    "#!pip install htop\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import joblib\n",
    "import sys\n",
    "\n",
    "import scipy.signal as sp\n",
    "from scipy.ndimage import median_filter\n",
    "from scipy.ndimage import uniform_filter\n",
    "import time\n",
    "import traceback\n",
    "from joblib.externals.loky import set_loky_pickler\n",
    "from joblib import parallel_backend\n",
    "from joblib import Parallel, delayed\n",
    "import scipy.stats as sps\n",
    "from joblib import wrap_non_picklable_objects\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#clf = DecisionTreeClassifier(random_state=0)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from joblib.externals.loky import set_loky_pickler\n",
    "from joblib import parallel_backend\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import wrap_non_picklable_objects\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading PD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_new.pkl\", \"rb\") as input_file:#(\"/home/jupyter/Adi Part 4/liver-train-val.pkl\", \"rb\") as input_file:case_data_train4#case_data_train1_3.pkl#case_data_train4.pkl\n",
    "    train=pd.read_pickle(input_file)#pickle.load(input_file)\n",
    "''' sanse: dataframe with patient information'''\n",
    "#train=pickle.load('case_data_train1_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['caseID', 'name', 'Patient_id', 'Repeated?', 'Site', 'Height', 'Weight',\n",
       "       'BMI', 'Age', 'Gender', 'History1', 'History2', 'History3',\n",
       "       'History hbv', 'History hcv', 'History (yes/no)', 'KPA',\n",
       "       'Fibrosis Grade', 'CAP', 'Steatosis Grade', 'Disease Labelled',\n",
       "       'Disease', 'Unlabelled Disease', 'Payable', 'irb', 'Ascites',\n",
       "       'Portal Vein Thrombosis', 'Biopsy', 'Jaundice', 'shadowing_test',\n",
       "       'Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1509, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseID</th>\n",
       "      <th>name</th>\n",
       "      <th>Disease</th>\n",
       "      <th>History</th>\n",
       "      <th>BMI</th>\n",
       "      <th>FibroscanCAP</th>\n",
       "      <th>FibroscanKPA</th>\n",
       "      <th>Country</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>number_of_lesions</th>\n",
       "      <th>lesion_size</th>\n",
       "      <th>Notes</th>\n",
       "      <th>shadowing_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PDG1556036195941</td>\n",
       "      <td>PDG1556036195941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PDG1556103181615</td>\n",
       "      <td>PDG1556103181615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>test</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PDG1556357075099</td>\n",
       "      <td>PDG1556357075099</td>\n",
       "      <td>[cirrhosis]</td>\n",
       "      <td>[hcv]</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PDG1556384740608</td>\n",
       "      <td>PDG1556384740608</td>\n",
       "      <td>[cirrhosis]</td>\n",
       "      <td>[hcv]</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>pending upload</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PDG1556440560110</td>\n",
       "      <td>PDG1556440560110</td>\n",
       "      <td>[cirrhosis]</td>\n",
       "      <td>[hcv]</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Egypt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             caseID              name      Disease History   BMI  \\\n",
       "0  PDG1556036195941  PDG1556036195941          NaN     NaN  22.0   \n",
       "1  PDG1556103181615  PDG1556103181615          NaN     NaN  19.0   \n",
       "2  PDG1556357075099  PDG1556357075099  [cirrhosis]   [hcv]  25.0   \n",
       "3  PDG1556384740608  PDG1556384740608  [cirrhosis]   [hcv]  25.0   \n",
       "4  PDG1556440560110  PDG1556440560110  [cirrhosis]   [hcv]  26.0   \n",
       "\n",
       "   FibroscanCAP  FibroscanKPA Country  Gender   Age  number_of_lesions  \\\n",
       "0           NaN           NaN   Egypt     1.0  65.0                NaN   \n",
       "1           NaN           NaN   Egypt     1.0  25.0                NaN   \n",
       "2           NaN           NaN   Egypt     1.0  67.0                NaN   \n",
       "3           NaN           NaN   Egypt     NaN  25.0                NaN   \n",
       "4           NaN           NaN   Egypt     NaN  78.0                NaN   \n",
       "\n",
       "   lesion_size           Notes  \\\n",
       "0          NaN             NaN   \n",
       "1          1.0            test   \n",
       "2          4.0             NaN   \n",
       "3         25.0  pending upload   \n",
       "4          5.0             NaN   \n",
       "\n",
       "                                      shadowing_test  \n",
       "0  [[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2  [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "''' sanse: Just checking out on tumorous diseases '''\n",
    "PD=copy.deepcopy(train)\n",
    "showy=train[((PD['Disease'].str.contains('Nodule', case = False)) | (PD['Disease'].str.contains('tumor', case = False)) |(PD['Disease'].str.contains('lesion', case = False)) |(PD['Disease'].str.contains('hcc', case = False)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PD['KPA']= [float(str(val).replace(' ','')) for val in PD['KPA'].values]\n",
    "PD['CAP']= [float(str(val).replace(' ','')) for val in PD['CAP'].values]\n",
    "''' sanse: Debug empty KPA and CAP values'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy=PD[(PD['KPA']<7) & ~((PD['Disease'].str.contains('Other', case = False))|(PD['Disease'].str.contains('hcc', case = False)))]#train[train['KPA']<7]\n",
    "#PD=train[(train['Label']==6) | (train['Label']==5)]#changed for now\n",
    "Fibtwo=PD[(PD['KPA']>=7)&(PD['KPA']<9) & ~((PD['Disease'].str.contains('Other', case = False))|(PD['Disease'].str.contains('hcc', case = False)))]\n",
    "\n",
    "Fibthree=PD[(PD['KPA']>=9)&(PD['KPA']<17) & ~((PD['Disease'].str.contains('Other', case = False))  |(PD['Disease'].str.contains('hcc', case = False)))]\n",
    "\n",
    "#PD=train[(train['Label']==6) | (train['Label']==5)]#\n",
    "Cirr=PD[(PD['KPA']>=17) &(PD['KPA']<80)& ~((PD['Disease'].str.contains('Other', case = False))  |(PD['Disease'].str.contains('hcc', case = False)))]\n",
    "\n",
    "'''sanse: OnX thresholds. Skip unclear clinical cases (cancers, nodules, etc.)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthynames2 = np.array(healthy['caseID'].tolist())\n",
    "Fibtwonames2=np.array(Fibtwo['caseID'].tolist())\n",
    "Fibthreenames2=np.array(Fibthree['caseID'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "Fibthreenames2=np.array(Fibthree['caseID'].tolist())\n",
    "Cirrnames2=np.array(Cirr['caseID'].tolist())\n",
    "c3=copy.deepcopy(Cirrnames2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cirrnames2=np.array(Cirr['caseID'].tolist())\n",
    "c3=copy.deepcopy(Cirrnames2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(530,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(healthynames2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f33=Fibthreenames2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233,)\n",
      "(40,)\n",
      "(65,)\n",
      "(55,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(h3))\n",
    "print(np.shape(f23))\n",
    "print(np.shape(f33))\n",
    "print(np.shape(c3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "c3k=np.arange(0,np.shape(c3)[0])\n",
    "f3k=np.arange(0,np.shape(f33)[0])\n",
    "f2k=np.arange(0,np.shape(f23)[0])\n",
    "h3k=np.arange(0,np.shape(h3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5, 8)\n",
      "(5, 13)\n",
      "(5, 11)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(h3k))\n",
    "print(np.shape(f2k))\n",
    "print(np.shape(f3k))\n",
    "print(np.shape(c3k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233,)\n",
      "(40,)\n",
      "(65,)\n",
      "(55,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(h3))\n",
    "print(np.shape(f23))\n",
    "print(np.shape(f33))\n",
    "print(np.shape(c3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c3k=np.arange(0,np.shape(c3)[0])\n",
    "'''sanse: indexing the patients'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3=healthynames2\n",
    "f23=Fibtwonames2\n",
    "f33=Fibthreenames2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3kpa=healthy['KPA'].tolist()\n",
    "f2kpa=Fibtwo['KPA'].tolist()\n",
    "f3kpa=Fibthree['KPA'].tolist()\n",
    "c3kpa=Cirr['KPA'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3cap=healthy['CAP'].tolist()\n",
    "f2cap=Fibtwo['CAP'].tolist()\n",
    "f3cap=Fibthree['CAP'].tolist()\n",
    "c3cap=Cirr['CAP'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3kpa=Fibthree['KPA'].tolist()\n",
    "c3kpa=Cirr['KPA'].tolist()\n",
    "f3cap=Fibthree['CAP'].tolist()\n",
    "c3cap=Cirr['CAP'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data has already been split, load the split\n",
    "c3=np.load('/home/jupyter/AdisStuff/kfold_new38/c3.npy' )#\n",
    "f33=np.load('/home/jupyter/AdisStuff/kfold_new38/f33.npy')#)\n",
    "f23=np.load('/home/jupyter/AdisStuff/kfold_new38/f23.npy')#)\n",
    "h3=np.load('/home/jupyter/AdisStuff/kfold_new38/h3.npy' )#\n",
    "\n",
    "c3k=np.load('/home/jupyter/AdisStuff/kfold_new38/c3k.npy',allow_pickle=True)\n",
    "f3k=np.load('/home/jupyter/AdisStuff/kfold_new38/f3k.npy',allow_pickle=True)\n",
    "f2k=np.load('/home/jupyter/AdisStuff/kfold_new38/f2k.npy',allow_pickle=True)\n",
    "h3k=np.load('/home/jupyter/AdisStuff/kfold_new38/h3k.npy',allow_pickle=True)\n",
    "'''sanse: Reload k-fold split (given split of patients). 5-fold crossvalidations'''\n",
    "'''sanse: c3, f33, f23, h3: names, c3k, f3k, f2k, h3k: indices for splitting'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variables are described below\n",
    "# video : a single entry of RF_list (from patient_loader) - RF_list is the RF (signal) data for all slices of all frames of all videos\n",
    "# threshold : the LOWER LIMIT of what is considered a normal difference between consecutive signal point, in non-shadowing data. A greater value means more leniency in assigning deadspots (potential shadow)\n",
    "# deadlength : the region size when searching for deadspots within a signal. 350 seems reasonable as each slice gives ~2800 data points\n",
    "# shift : the shift applied to regions when scanning through a slice (signal) in search of deadspots. A greater shift means less overlap between consecutive regions. Some overlap is needed as a safety net\n",
    "# deadavg : used on slices, when looking at regions of size deadlength, with values of 0 (dead) or 1 (alive) for each data point. An average along the region of below \"deadavg\" implies a deadspot\n",
    "# shadowing_threshold: threshold for % area of frame that is shadowing\n",
    "\n",
    "def video_shadowing_quicker( video, threshold, deadlength, shift, deadavg, shadowing_threshold ):\n",
    "    '''sanse: % of each frame that has shadow. it is validated. its based on data science (intern Anton / Breah) '''\n",
    "\n",
    "    signal_threshold = 100\n",
    "    \n",
    "    videoshape = np.shape(video)\n",
    "    numFrames = videoshape[0]\n",
    "    slices = videoshape[2]\n",
    "    numPoints = videoshape[1]\n",
    "    videoDeadFrames = np.zeros(numFrames)\n",
    "    \n",
    "    #number of regions used for each signal\n",
    "    sizeup = numPoints - 1\n",
    "    numRegions = (int)(1 + (int) ( (sizeup-deadlength)/shift ) )\n",
    "    \n",
    "    deadspots = np.zeros((numFrames , slices , numRegions ))\n",
    "    deadspots_disp = np.zeros(( numFrames , numPoints, slices ))\n",
    "    \n",
    "    #first, find difference between each pair of consecutive points\n",
    "    diff = np.diff( video , axis = 1 )\n",
    "    \n",
    "    #organize each difference as a 1 or 0, wrt some threshold difference we would expect for non-shadowing data\n",
    "    \n",
    "    thresh = 1.0 * ( np.abs( diff ) > threshold)\n",
    "    \n",
    "    startspot = np.array( [ (shift*ii) for ii in range(0, numRegions) ] )\n",
    "    endspot = np.add( startspot , deadlength )\n",
    "    endspot[numRegions-1] = sizeup\n",
    "    \n",
    "    avgLife = np.zeros( (numFrames , numRegions , slices) )\n",
    "    \n",
    "    for kk in range (0, numFrames):\n",
    "        avgLife[ kk , :, : ] = np.array( [ [ (np.mean( thresh[ kk , startspot[ii] : endspot[ii]  , jj ] )) for jj in range (0, slices) ] for ii in range( 0 , numRegions ) ]  )\n",
    "    \n",
    "    deadspot = np.array(  [ [ [ (( avgLife[kk, ii, jj] < deadavg ) and ( np.mean(np.abs(video[kk, startspot[ii]:endspot[ii]+1 , jj])) < signal_threshold )) for ii in range( 0, numRegions) ] for jj in range(0, slices)]  for kk in range( 0, numFrames) ] )\n",
    "    deadspot = np.multiply( deadspot , 1 )\n",
    "    # make an array that simplifies the display of the deadspaces -> if the regions is dead, fill it with 1s\n",
    "    deadspots_disp = np.zeros( (numFrames, numPoints , slices) )\n",
    "    \n",
    "    #fill in deadspots_disp\n",
    "    for kk in range (0, numFrames):\n",
    "        for jj in range( 0 , slices ):\n",
    "            for ii in range( 0 , numRegions ):\n",
    "                if (deadspot[kk, jj , ii] == 1):\n",
    "                    deadspots_disp[kk, startspot[ii] :endspot[ii] , jj ] = 1\n",
    "    \n",
    "    total = slices*numPoints\n",
    "    \n",
    "    dead = np.zeros(numFrames)\n",
    "    shadowing_fraction = np.zeros(numFrames)\n",
    "    for kk in range (0, numFrames):\n",
    "        dead[kk] = np.sum(deadspots_disp[kk,:,:])\n",
    "        shadowing_fraction[kk] = dead[kk]/total\n",
    "    \n",
    "    # ceil to 2 decimal places\n",
    "    shadowing_fraction = 0.01 * (np.ceil( np.multiply(100.0,shadowing_fraction) ))\n",
    "    #frame_check = [ 1.0*(shadowing_fraction[ii] < shadowing_threshold) for ii in range(0, numFrames) ]## what I need\n",
    "    '''\n",
    "    video_check = np.sum(frame_check) / numFrames\n",
    "    \n",
    "    # now search for regions of the video ( size aka # of frames = 0.3 * total # of frames )\n",
    "    # where 8-9/10 frames are good -> ie. the avg of framecheck over these frames needs to be 0.8-0.9 or greater\n",
    "    chunkFrames = (int)(np.ceil( 0.3*numFrames ))\n",
    "    numChunks = 1 + numFrames - chunkFrames\n",
    "    chunkFound = 0\n",
    "    counter = (int)(0)\n",
    "    avgQual = 0\n",
    "    avgShad = 1  \n",
    "    \n",
    "    # v2.0 - Looks for a region with shadowing_threshold or better shadowing on average\n",
    "    while ( (avgShad >= shadowing_threshold) and (counter < numChunks) ):\n",
    "        avgShad = np.mean( shadowing_fraction[counter:counter+chunkFrames] )\n",
    "        counter += 1\n",
    "    if ( avgShad <= shadowing_threshold ):\n",
    "        chunkFound = 1\n",
    "    '''\n",
    "    return shadowing_fraction#actually is frame_check#video_check, chunkFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input variables are described below\n",
    "# video : a single entry of RF_list (from patient_loader) - RF_list is the RF (signal) data for all slices of all frames of all videos\n",
    "# threshold : the LOWER LIMIT of what is considered a normal difference between consecutive signal point, in non-shadowing data. A greater value means more leniency in assigning deadspots (potential shadow)\n",
    "# deadlength : the region size when searching for deadspots within a signal. 350 seems reasonable as each slice gives ~2800 data points\n",
    "# shift : the shift applied to regions when scanning through a slice (signal) in search of deadspots. A greater shift means less overlap between consecutive regions. Some overlap is needed as a safety net\n",
    "# deadavg : used on slices, when looking at regions of size deadlength, with values of 0 (dead) or 1 (alive) for each data point. An average along the region of below \"deadavg\" implies a deadspot\n",
    "# shadowing_threshold: threshold for % area of frame that is shadowing\n",
    "\n",
    "def video_shadowing_quicker2( video, threshold, deadlength, shift, deadavg, shadowing_threshold ):\n",
    "    '''sanse: give a threshold and returns the binary (quality check on frames aspect)'''\n",
    "\n",
    "    signal_threshold = 100\n",
    "    \n",
    "    videoshape = np.shape(video)\n",
    "    numFrames = videoshape[0]\n",
    "    slices = videoshape[2]\n",
    "    numPoints = videoshape[1]\n",
    "    videoDeadFrames = np.zeros(numFrames)\n",
    "    \n",
    "    #number of regions used for each signal\n",
    "    sizeup = numPoints - 1\n",
    "    numRegions = (int)(1 + (int) ( (sizeup-deadlength)/shift ) )\n",
    "    \n",
    "    deadspots = np.zeros((numFrames , slices , numRegions ))\n",
    "    deadspots_disp = np.zeros(( numFrames , numPoints, slices ))\n",
    "    \n",
    "    #first, find difference between each pair of consecutive points\n",
    "    diff = np.diff( video , axis = 1 )\n",
    "    \n",
    "    #organize each difference as a 1 or 0, wrt some threshold difference we would expect for non-shadowing data\n",
    "    \n",
    "    thresh = 1.0 * ( np.abs( diff ) > threshold)\n",
    "    \n",
    "    startspot = np.array( [ (shift*ii) for ii in range(0, numRegions) ] )\n",
    "    endspot = np.add( startspot , deadlength )\n",
    "    endspot[numRegions-1] = sizeup\n",
    "    \n",
    "    avgLife = np.zeros( (numFrames , numRegions , slices) )\n",
    "    \n",
    "    for kk in range (0, numFrames):\n",
    "        avgLife[ kk , :, : ] = np.array( [ [ (np.mean( thresh[ kk , startspot[ii] : endspot[ii]  , jj ] )) for jj in range (0, slices) ] for ii in range( 0 , numRegions ) ]  )\n",
    "    \n",
    "    deadspot = np.array(  [ [ [ (( avgLife[kk, ii, jj] < deadavg ) and ( np.mean(np.abs(video[kk, startspot[ii]:endspot[ii]+1 , jj])) < signal_threshold )) for ii in range( 0, numRegions) ] for jj in range(0, slices)]  for kk in range( 0, numFrames) ] )\n",
    "    deadspot = np.multiply( deadspot , 1 )\n",
    "    # make an array that simplifies the display of the deadspaces -> if the regions is dead, fill it with 1s\n",
    "    deadspots_disp = np.zeros( (numFrames, numPoints , slices) )\n",
    "    \n",
    "    #fill in deadspots_disp\n",
    "    for kk in range (0, numFrames):\n",
    "        for jj in range( 0 , slices ):\n",
    "            for ii in range( 0 , numRegions ):\n",
    "                if (deadspot[kk, jj , ii] == 1):\n",
    "                    deadspots_disp[kk, startspot[ii] :endspot[ii] , jj ] = 1\n",
    "    \n",
    "    total = slices*numPoints\n",
    "    \n",
    "    dead = np.zeros(numFrames)\n",
    "    shadowing_fraction = np.zeros(numFrames)\n",
    "    for kk in range (0, numFrames):\n",
    "        dead[kk] = np.sum(deadspots_disp[kk,:,:])\n",
    "        shadowing_fraction[kk] = dead[kk]/total\n",
    "    \n",
    "    # ceil to 2 decimal places\n",
    "    shadowing_fraction = 0.01 * (np.ceil( np.multiply(100.0,shadowing_fraction) ))\n",
    "    frame_check = [ 1.0*(shadowing_fraction[ii] < shadowing_threshold) for ii in range(0, numFrames) ]## what I need\n",
    "    '''\n",
    "    video_check = np.sum(frame_check) / numFrames\n",
    "    \n",
    "    # now search for regions of the video ( size aka # of frames = 0.3 * total # of frames )\n",
    "    # where 8-9/10 frames are good -> ie. the avg of framecheck over these frames needs to be 0.8-0.9 or greater\n",
    "    chunkFrames = (int)(np.ceil( 0.3*numFrames ))\n",
    "    numChunks = 1 + numFrames - chunkFrames\n",
    "    chunkFound = 0\n",
    "    counter = (int)(0)\n",
    "    avgQual = 0\n",
    "    avgShad = 1  \n",
    "    \n",
    "    # v2.0 - Looks for a region with shadowing_threshold or better shadowing on average\n",
    "    while ( (avgShad >= shadowing_threshold) and (counter < numChunks) ):\n",
    "        avgShad = np.mean( shadowing_fraction[counter:counter+chunkFrames] )\n",
    "        counter += 1\n",
    "    if ( avgShad <= shadowing_threshold ):\n",
    "        chunkFound = 1\n",
    "    '''\n",
    "    return frame_check#video_check, chunkFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanconvert(Iin=None, info=None):#Apitch, Lpitch, Radius, PixelsPerMM)\n",
    "    '''sanse: same as in MaskingCleaned'''\n",
    "    import numpy as np\n",
    "    if (info[\"Radius\"] == 0):    # A linear array probe\n",
    "        print(\"Linear Array\")\n",
    "        AxialExtent = info[\"Apitch\"] * np.size(Iin, 0)\n",
    "        LateralExtent = info[\"Lpitch\"] * np.size(Iin, 1)\n",
    "        AxialSize = AxialExtent * 1000 * info[\"PixelsPerMM\"]\n",
    "        LateralSize = LateralExtent * 1000 * info[\"PixelsPerMM\"]\n",
    "        Iout = np.resize(Iin, (LateralSize, AxialSize))    # cubic interpolation\n",
    "    else:    # A curved array probe    \n",
    "        t = ((np.arange(1,Iin.shape[1],dtype=int)) - Iin.shape[1]/ 2) * info[\"Lpitch\"] / info[\"Radius\"]\n",
    "        r = info[\"Radius\"] + (np.arange(1,Iin.shape[0])) * info[\"Apitch\"]\n",
    "        [t, r] = np.meshgrid(t, r)\n",
    "        x = np.multiply(r, np.cos(t))\n",
    "        y = np.multiply(r, np.sin(t))\n",
    "    \n",
    "        divides=1e-3 / info[\"PixelsPerMM\"]\n",
    "        xMin=np.min(x)\n",
    "        xMax=np.max(x)\n",
    "        #print(\"X Min : %s , X Max %s\" % (xMin,xMax))\n",
    "        #print(\"Divides : %s \" % divides)\n",
    "        xreg =  np.arange(np.min(x) , np.max(x) ,divides)\n",
    "        yreg =  np.arange(np.min(y) , np.max(y) ,divides)\n",
    "        [yreg, xreg] = np.meshgrid(yreg, xreg) \n",
    "        Iout = np.zeros(xreg.shape)\n",
    "        xCntr = np.arange(0,xreg.shape[1])\n",
    "        Cntry = np.arange(0,yreg.shape[0])\n",
    "        theta = np.array([cart4pol(xreg[yCntr, xCntr], yreg[yCntr, xCntr]) for yCntr in Cntry])\n",
    "        rho = np.array([cart2pol(xreg[yCntr, xCntr], yreg[yCntr, xCntr]) for yCntr in Cntry])\n",
    "        #for xCntr in tqdm(np.arange(0,xreg.shape[1])):\n",
    "            #for yCntr in np.arange(0,yreg.shape[0]):\n",
    "                #[theta, rho] = cart2pol(xreg[yCntr, xCntr], yreg[yCntr, xCntr])\n",
    "                #print(\"theta : %s , rho %s\" % (theta,rho))\n",
    "        indt = np.floor(theta / (info[\"Lpitch\"] / info[\"Radius\"]) + Iin.shape[1] / 2) \n",
    "        indr = np.floor((rho - info[\"Radius\"]) / info[\"Apitch\"]) \n",
    "\n",
    "        [j,i]=((indt>=0)&(indt <= t.shape[1])& (indr>= 0) &(indr <= r.shape[0])).nonzero()\n",
    "        #print(np.size(j))\n",
    "        j=j.astype(int)\n",
    "        i=i.astype(int)\n",
    "        Iout[j, i] = Iin[indr[j,i].astype(int), indt[j,i].astype(int)]\n",
    "        #for i in xCntr:\n",
    "            \n",
    "         #   for j in Cntry:\n",
    "                \n",
    "          #      if indt[j,i] >= 0 and (indt[j,i] <= t.shape[1]and (indr[j,i] >= 0 and indr[j,i] <= r.shape[0])):\n",
    "                    \n",
    "           #         Iout[int(Cntry[j]), int(xCntr[i])] = Iin[int(indr[j,i]), int(indt[j,i])]\n",
    "    #print(np.shape(Iout))\n",
    "    return Iout\n",
    "\n",
    "def cart2pol(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    return rho\n",
    "def cart4pol(x, y):\n",
    "    phi = np.arctan2(y, x)\n",
    "    return phi\n",
    "\n",
    "\n",
    "def pol2cart(rho, phi):\n",
    "    x = rho * np.cos(phi)\n",
    "    y = rho * np.sin(phi)\n",
    "    return(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf2bmode(rf):\n",
    "    '''sanse: same as DSP_Part1'''\n",
    "    import numpy as np\n",
    "    from scipy.signal import hilbert,decimate,resample\n",
    "\n",
    "\n",
    "    # decimation factor use to create Env from RF\n",
    "    decimationFactor = 1\n",
    "\n",
    "    # make a compression table\n",
    "    alpha = 0.55# sqrt compression, change for a different compression table\n",
    "    denom = np.exp(alpha * np.log(65535)) / 255.0\n",
    "    CompressionTable = np.exp(alpha * np.log(np.arange(0,65536))) / denom \n",
    "    # CompressionTable = np.exp(alpha * np.log(65535)) / denom\n",
    "    \n",
    "    # calculate envelope and log compress\n",
    "    Env = 1 + np.fix(np.abs(hilbert(rf, axis= 0)))\n",
    "    Env = CompressionTable[np.array(Env, dtype=int)]\n",
    "\n",
    "    EnvDec =resample(Env ,int(len(Env[ : , 0])/decimationFactor))\n",
    "    \n",
    "    EnvDec[(EnvDec>255).nonzero()]=255\n",
    "#EnvDec[(EnvDec>255).nonzero()]=255\n",
    "    return EnvDec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"BackScatter Coefficient Rectangular Window\"))-0\\n \"ESD Rectangular Window\"))-1\\n     \"EAC Rectangular Window\"))-2\\n \"Spectral Slope Rectangular Window\"))-3\\n    \"Spectral Intercept Rectangular Window\"))-4\\n  \"MidBand Fit Rectangular Window\"))-5\\n \"Attenuation Coefficient Hamming Window\"))-6\\n\"BackScatter Coefficient Hamming Window\"))-7\\n \"ESD Hamming Window\"))-8\\n \"EAC Hamming Window\"))-9\\n \"Spectral Slope Hamming Window\"))-10\\n\"Spectral Intercept Hamming Window\"))-11\\n \"MidBand Fit Hamming Window\"))-12\\n      \"Nakagami Spread\"))-13\\n \"Nakagami Shape\"-14\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\"BackScatter Coefficient Rectangular Window\"))-0\n",
    " \"ESD Rectangular Window\"))-1\n",
    "     \"EAC Rectangular Window\"))-2\n",
    " \"Spectral Slope Rectangular Window\"))-3\n",
    "    \"Spectral Intercept Rectangular Window\"))-4\n",
    "  \"MidBand Fit Rectangular Window\"))-5\n",
    " \"Attenuation Coefficient Hamming Window\"))-6\n",
    "\"BackScatter Coefficient Hamming Window\"))-7\n",
    " \"ESD Hamming Window\"))-8\n",
    " \"EAC Hamming Window\"))-9\n",
    " \"Spectral Slope Hamming Window\"))-10\n",
    "\"Spectral Intercept Hamming Window\"))-11\n",
    " \"MidBand Fit Hamming Window\"))-12\n",
    "      \"Nakagami Spread\"))-13\n",
    " \"Nakagami Shape\"-14\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CODE TO USE NEW QUS PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to access the buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Bucket: train-new>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudstorage as gcs\n",
    "from google.cloud import storage\n",
    "storage_client = storage.Client()\n",
    "\n",
    "bucket_pull_name =\"train-new\"#\"liver-data-train-4-n\"#\"liver-data-48x61-alldata-test-1\"#\"liver-data-train-val\" \"liver-data-48x61-alldata-train-n\"#\n",
    "\n",
    "bucket_pull = storage_client.get_bucket(bucket_pull_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_video(data_type):\n",
    "    length = len(data_type) + 3\n",
    "    # get all patient IDS - use the rf VIDEO folder for simplicity\n",
    "    # get the blob iterator objects\n",
    "    blob = bucket_pull.list_blobs()\n",
    "    # get relevant file names from blob objects\n",
    "    fileList1 = [file.name for file in blob if ( file.name.endswith((data_type+'.npy')) and file.name.startswith((data_type+'_v/')) ) ]\n",
    "    fileList1 = np.asarray(fileList1)\n",
    "    size = np.size(fileList1)\n",
    "    ID_list1 = np.copy(fileList1)\n",
    "    for ii in range(0, size):\n",
    "        ID_list1[ii] = fileList1[ii][length:length+16]\n",
    "    # avoid duplicates\n",
    "    ID_list1 = np.unique(ID_list1)\n",
    "    # get all video names\n",
    "    size = np.shape(fileList1)[0]\n",
    "    fileList = np.copy(fileList1)\n",
    "    caseID = np.copy(fileList)\n",
    "    videID = np.copy(fileList)\n",
    "    bothID = np.copy(fileList)\n",
    "    for ii in range(0, size):\n",
    "        fileList[ii] = fileList1[ii][length:]\n",
    "        split = fileList[ii].split('_')\n",
    "        caseID[ii] = split[0]\n",
    "        videID[ii] = split[1]\n",
    "        bothID[ii] = split[0] + '_' + split[1]\n",
    "    caseID = np.unique(caseID)\n",
    "    videID = np.unique(videID)\n",
    "    bothID2 = np.unique(bothID)\n",
    "    return caseID, videID, bothID2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseID, videID, bothID2 = get_info_video('qus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get videos and frame numbers for each case\n",
    "info_list = []\n",
    "\n",
    "for ii in range(0, np.shape(caseID)[0]):\n",
    "    # set up info list\n",
    "    info_list.append( [] )\n",
    "    \n",
    "    # get ID\n",
    "    info_list[ii].append(caseID[ii])\n",
    "    \n",
    "    # get video list\n",
    "    vidlist = [ jj for jj in bothID2 if caseID[ii] in jj ]\n",
    "    vidlist =  [ jj[17:] for jj in vidlist ]\n",
    "    info_list[ii].append(vidlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = {}\n",
    "case_dict = {}\n",
    "for ii in range(0, len(info_list)):\n",
    "    \n",
    "    case_dict = {'videos': info_list[ii][1] }\n",
    "    \n",
    "    info_dict[info_list[ii][0]] = case_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load video directly from _v/ folder\n",
    "def load_qus_video_direct( ID , vid ):   \n",
    "    filename = 'qus/' + ID + '_' + vid + '_qus.npy' \n",
    "    temp = storage.blob.Blob(filename,bucket_pull)\n",
    "    content = temp.download_as_string()\n",
    "    video = np.load(BytesIO(content))\n",
    "    return video\n",
    "\n",
    "# load video directly from _v/ folder\n",
    "def load_rf_video_direct( ID , vid ):   \n",
    "    filename = 'rf/' + ID + '_' + vid + '_rf.npy' \n",
    "    temp = storage.blob.Blob(filename,bucket_pull)\n",
    "    content = temp.download_as_string()\n",
    "    video = np.load(BytesIO(content))\n",
    "    return video\n",
    "\n",
    "# load video directly from _v/ folder\n",
    "def load_quant_video_direct( ID , vid ):   \n",
    "    filename = 'quant/' + ID + '_' + vid + '_quant.npy' \n",
    "    temp = storage.blob.Blob(filename,bucket_pull)\n",
    "    content = temp.download_as_string()\n",
    "    video = np.load(BytesIO(content))\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load video directly from _v/ folder\n",
    "def load_qus_video_direct2( ID , vid ):   \n",
    "    filename = 'qus_v/' + ID + '_' + vid + '_qus.npy' \n",
    "    temp = storage.blob.Blob(filename,bucket_pull)\n",
    "    content = temp.download_as_string()\n",
    "    video = np.load(BytesIO(content))\n",
    "    return video\n",
    "\n",
    "# load video directly from _v/ folder\n",
    "def load_rf_video_direct2( ID , vid ):   \n",
    "    filename = 'rf_v/' + ID + '_' + vid + '_rf.npy' \n",
    "    temp = storage.blob.Blob(filename,bucket_pull)#bucket_pull_2\n",
    "    content = temp.download_as_string()\n",
    "    video = np.load(BytesIO(content))\n",
    "    return video\n",
    "\n",
    "# load video directly from _v/ folder\n",
    "def load_quant_video_direct2( ID , vid ):   \n",
    "    filename = 'quant_v/' + ID + '_' + vid + '_quant.npy' \n",
    "    temp = storage.blob.Blob(filename,bucket_pull)\n",
    "    content = temp.download_as_string()\n",
    "    video = np.load(BytesIO(content))\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket_pull_name_2 = \"liver-data-48x61-alldata-train\"#\"\"liver-data-train-4\"#\"liver-data-48x61-alldata-test-1\"#\"liver-data-train-val\" \"liver-data-48x61-alldata-train\"#\"\n",
    "\n",
    "# bucket_pull_2 = storage_client.get_bucket(bucket_pull_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket_pull_name_3 = \"liver-data-48x61-alldata-train-n-3\"#\"liver-data-48x61-alldata-train-n\"#\"\"liver-data-train-4-n\"#\"liver-data-48x61-alldata-test-1-n\"#\"liver-data-train-val\" \"liver-data-48x61-alldata-train-n\"#\"\n",
    "\n",
    "# bucket_pull_3 = storage_client.get_bucket(bucket_pull_name_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'newmodel'+files[m][2:-6]+'msk.npy') ), 'w') , predicted_mask_good )\n",
    "# 'prctmodel'+files[m][2:-6]+'msk.npy') ), 'w') , predicted_mask_bad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fullmask_video_direct2( ID , vid ):   \n",
    "    #filename = 'newmodel_v/' + ID + '_' + vid + '_msk.npy' #switch new for old newmodel_v\n",
    "    filename = 'bigmask_v/' + ID + '_' + vid + '_msk.npy'\n",
    "    temp = storage.blob.Blob(filename,bucket_pull)#bucket_pull_3\n",
    "    content = temp.download_as_string()\n",
    "    video = np.load(BytesIO(content))\n",
    "    return video\n",
    "def load_prcntmask_video_direct2( ID , vid ):   \n",
    "    #filename = 'prctmodel_v/' + ID + '_' + vid + '_msk.npy' \n",
    "    filename = 'origmsk_v/' + ID + '_' + vid + '_msk.npy' #\n",
    "    temp = storage.blob.Blob(filename,bucket_pull)#_3\n",
    "    content = temp.download_as_string()\n",
    "    video = np.load(BytesIO(content))\n",
    "    return video\n",
    "def load_ps_video_direct2( ID , vid ):   \n",
    "    #filename = 'prctmodel_v/' + ID + '_' + vid + '_msk.npy' \n",
    "    filename = 'ps_v/' + ID + '_' + vid + '_ps.npy' #\n",
    "    temp = storage.blob.Blob(filename,bucket_pull)\n",
    "    content = temp.download_as_string()\n",
    "    video = np.load(BytesIO(content))\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If the data hasn't been split into fold it happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_fold_split(a3,k):\n",
    "    '''sanse: k-fold split. It is stratified in advance, and the split is then done for each disease individually.'''\n",
    "    cor=np.arange(0,np.shape(a3)[0])\n",
    "    cora=copy.deepcopy(cor)\n",
    "    random.shuffle(cora)\n",
    "    f45=np.array_split(cora,k)\n",
    "    return f45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "c3k=K_fold_split(c3,5)\n",
    "f3k=K_fold_split(f33,5)\n",
    "f2k=K_fold_split(f23,5)\n",
    "h3k=K_fold_split(h3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([227,  51, 171,  32,  28,  88,  11,  35, 166, 164, 107, 201,  39,\n",
      "       129, 150,  95, 218, 169,  93,  89,  25, 183,  72,  14, 118, 136,\n",
      "        20,  85, 211,  65,  61,  18, 231, 190, 225,   1,  94, 187, 147,\n",
      "       148, 216, 195,  24, 139, 210,  90, 145]), array([109,   9,  41, 101, 175, 191, 149, 165,  31, 144,  87, 137, 162,\n",
      "       161, 167,  83, 177, 185,  80, 212,  19, 113,  38, 214,  30,  76,\n",
      "        53, 194,  75, 117,   0,  27,  56,  84, 172,  49, 178,  55, 116,\n",
      "       138,  99, 134,  96,  68, 203,  37, 141]), array([ 26,  74, 170,  69, 102,  81, 143, 221, 160, 112,  36, 209,  59,\n",
      "       142, 224, 104, 124,  33,  97, 205,  67, 200, 151,  16,  40,  60,\n",
      "        22,  44, 222, 146,  12, 131,  86, 230, 188,  91,  73, 121, 173,\n",
      "       135, 168, 229,  45, 213, 217,  29, 197]), array([228,  52, 123, 202, 226, 220,  34,  46, 127,   8,  21,   2, 155,\n",
      "       179, 100, 184, 199,  48, 176, 106,  57,  42,   6, 163, 152, 182,\n",
      "        17,  54,  58,  79, 157, 154,  82, 130,  23, 105, 207,  50,  10,\n",
      "        64, 192,  92,  98, 196, 223, 140]), array([156,  66, 208,   5, 115, 122,   3,  70, 159, 174,  43, 158, 204,\n",
      "        63,  78, 153, 206, 110, 232, 120, 108, 180, 103, 133, 126,  47,\n",
      "       132,   4, 111,  13, 186,  77,  71, 215, 219,   7, 189, 128, 125,\n",
      "       119,  15, 193, 181, 114, 198,  62])]\n",
      "[array([11, 10, 14, 20,  6, 35,  4, 19]), array([39, 32, 22, 37, 29, 18, 31, 34]), array([38,  3,  9, 28, 33,  7,  2, 36]), array([17, 27, 24, 26, 12,  0,  1, 15]), array([ 8, 23, 16, 25, 13, 21,  5, 30])]\n",
      "[array([17, 57, 41, 11, 18, 22, 21, 61, 52, 35, 36, 19,  2]), array([58, 63,  1, 64,  4, 60, 23, 27, 25, 54,  7, 55, 43]), array([10, 45, 29,  6,  3, 39, 28,  8, 37, 53, 24, 15, 49]), array([42,  5,  0, 14, 12, 46, 48, 50, 34, 30, 26, 32, 20]), array([47, 51, 40, 13, 31, 59, 33, 62, 44, 16, 38, 56,  9])]\n",
      "[array([22, 40,  5, 28, 44, 46, 37,  2,  6, 42, 18]), array([47,  3, 15, 54, 10, 43, 50, 20, 38, 11, 45]), array([49, 41,  7, 34,  1, 39, 21, 14,  4, 53, 23]), array([32, 12, 52, 36, 33, 25, 24, 48, 29, 35, 51]), array([27,  8,  0, 19, 13, 17, 26, 16, 31,  9, 30])]\n"
     ]
    }
   ],
   "source": [
    "print(h3k)\n",
    "print(f2k)\n",
    "print(f3k)\n",
    "print(c3k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(f33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106,)\n",
      "(19,)\n",
      "(24,)\n",
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(h3k[1]))\n",
    "print(np.shape(f2k[1]))\n",
    "print(np.shape(f3k[1]))\n",
    "print(np.shape(c3k[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This section is for a particular version of the GLCM, it isn't used now, but I kept it for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##important original\n",
    "''' GLCM requires discretized data, we pull 8 random patients for defining bins. Not anymore actual code. Now every patient has their own bins'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "qusfixingnames=np.hstack((h3[h3k[0][20]],h3[h3k[4][100]] ,f23[f2k[1][2]],f23[f2k[3][10]]  ,f33[f3k[1][3]],f33[f3k[2][20]]  ,c3[c3k[0][7]],c3[c3k[1][14]]          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##testing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "qusfixingnames=np.hstack((h3[h3k[0][20]],h3[h3k[4][60]] ,f23[f2k[1][2]],f23[f2k[3][8]]  ,f33[f3k[1][3]],f33[f3k[2][10]]  ,c3[c3k[0][2]],c3[c3k[1][1]]          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(h3k[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(qusfixingnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def qusforquant(patient_list, patient_indexes):\n",
    "    qusfout=[]\n",
    "    \n",
    "    for m in tqdm_notebook(range(0,np.shape(patient_indexes)[0])): \n",
    "        #DF = Load_QUS_Loc(c3[cr[m]:cr[m]+1])\n",
    "        #try:\n",
    "        #    DF['Case']\n",
    "        #    print(1)\n",
    "        #except:\n",
    "        #    continue\n",
    "        '''\n",
    "        try:\n",
    "            QUS_list, QUANT_list, RF_list = Dat_nump_loader(patient_list[patient_indexes[m]])\n",
    "        except:\n",
    "            try:\n",
    "                QUS_list, QUANT_list, RF_list = Dat_nump_loaderval(patient_list[patient_indexes[m]])\n",
    "            except:\n",
    "                print(patient_list[patient_indexes[m]])\n",
    "        '''\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        \n",
    "        try:\n",
    "            with parallel_backend('multiprocessing'):\n",
    "                QUS_list = Parallel(n_jobs=num_cores,mmap_mode='r+')(delayed(load_qus_video_direct)(patient_list[patient_indexes[m]],info_dict[patient_list[patient_indexes[m]]]['videos'][ii]) for ii in range(0,len(info_dict[patient_list[patient_indexes[m]]]['videos'])) )\n",
    "            #print('QUS_list shape', np.shape(QUS_list)[0])\n",
    "            \n",
    "            with parallel_backend('multiprocessing'):\n",
    "                RF_list = Parallel(n_jobs=num_cores,mmap_mode='r+')(delayed(load_rf_video_direct)(patient_list[patient_indexes[m]],info_dict[patient_list[patient_indexes[m]]]['videos'][ii]) for ii in range(0,len(info_dict[patient_list[patient_indexes[m]]]['videos'])) )\n",
    "            #print('RF_list shape', np.shape(RF_list)[0])\n",
    "           \n",
    "            #with parallel_backend('multiprocessing'):\n",
    "            #    QUANT_list = Parallel(n_jobs=num_cores,mmap_mode='r+')(delayed(load_quant_video_direct)(patient_list[patient_indexes[m]],info_dict[patient_list[patient_indexes[m]]]['videos'][ii]) for ii in range(0,len(info_dict[patient_list[patient_indexes[m]]]['videos'])) )\n",
    "        except:\n",
    "        \n",
    "            with parallel_backend('multiprocessing'):\n",
    "                QUS_list = Parallel(n_jobs=num_cores,mmap_mode='r+')(delayed(load_qus_video_direct2)(patient_list[patient_indexes[m]],info_dict[patient_list[patient_indexes[m]]]['videos'][ii]) for ii in range(0,len(info_dict[patient_list[patient_indexes[m]]]['videos'])) )\n",
    "            #print('QUS_list shape', np.shape(QUS_list)[0])\n",
    "            \n",
    "            with parallel_backend('multiprocessing'):\n",
    "                RF_list = Parallel(n_jobs=num_cores,mmap_mode='r+')(delayed(load_rf_video_direct2)(patient_list[patient_indexes[m]],info_dict[patient_list[patient_indexes[m]]]['videos'][ii]) for ii in range(0,len(info_dict[patient_list[patient_indexes[m]]]['videos'])) )\n",
    "            #print('RF_list shape', np.shape(RF_list)[0])\n",
    "           \n",
    "            #with parallel_backend('multiprocessing'):\n",
    "            #    QUANT_list = Parallel(n_jobs=num_cores,mmap_mode='r+')(delayed(load_quant_video_direct2)(patient_list[patient_indexes[m]],info_dict[patient_list[patient_indexes[m]]]['videos'][ii]) for ii in range(0,len(info_dict[patient_list[patient_indexes[m]]]['videos'])) )\n",
    "        \n",
    "        \n",
    "        #print('QUANT_list shape', np.shape(QUANT_list)[0])\n",
    "        # load the videos\n",
    "        #with parallel_backend('multiprocessing'):\n",
    "            #QUS_list = Parallel(n_jobs=num_cores,mmap_mode='r+')(delayed(load_qus_video)(patient_list[patient_indexes[m]],info_dict[patient_list[patient_indexes[m]]]['videos'][ii],info_dict[patient_list[patient_indexes[m]]]['frames'][ii]) for ii in range(0,len(info_dict[patient_list[patient_indexes[m]]]['videos'])) )\n",
    "        #print('QUANT_list shape', np.shape(QUANT_list)[0])\n",
    "        #print('QUS_list shape', np.shape(QUS_list)[0])\n",
    "        \n",
    "        #QUS_list, QUANT_list, RF_list, File_names = numpy_for_ID_seg(DF,c3[cr[m]])\n",
    "        #nums=Cirr['2'][Cirr['PDG1556357075099']==c3[cr[m]]].tolist()\n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "        for r in range(0,np.shape(RF_list)[0]):#(np.asarray(nums)-1):#range(0,np.shape(QUANT_list)[0]):\n",
    "            prep=video_shadowing_quicker2((RF_list[r]), 80, 350, 200, 0.08, 0.18) \n",
    "            \n",
    "            if np.shape(RF_list[r])[0]==0:\n",
    "                datavidtopat[np.shape(datavidtopat)[0]-1]=datavidtopat[np.shape(datavidtopat)[0]-1]-1\n",
    "                continue\n",
    "            elif np.max(prep)==0:\n",
    "                continue\n",
    "            Rf_list2=[]\n",
    "            Quant_list2=[]\n",
    "            Qus_list2=[]\n",
    "            \n",
    "            Qus_list2=QUS_list[r][(np.reshape(prep,(-1,)).nonzero()[0]),:,:]\n",
    "            qusfout.append(Qus_list2)\n",
    "    return qusfout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a862384fa3d74165b21ab20b95e9be4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "yki=qusforquant(qusfixingnames,np.arange(0,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(yki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "yki2=yki[0]\n",
    "for m in range(1,51):\n",
    "    yki2=np.vstack((yki2,yki[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1203, 1152, 17)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(yki2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "yki3=yki2[0,:,:]\n",
    "for m in range(1,1203):\n",
    "    yki3=np.vstack((yki3,yki2[m,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1385856, 17)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(yki3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1385856, 17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e010c66efb3d4aa4bb1245c2f7bee456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "yki=qusforquant(qusfixingnames,np.arange(0,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(yki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "yki2=yki[0]\n",
    "for m in range(1,51):\n",
    "    yki2=np.vstack((yki2,yki[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1203, 2304, 17)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(yki2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "yki3=yki2[0,:,:]\n",
    "for m in range(1,1203):\n",
    "    yki3=np.vstack((yki3,yki2[m,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2771712, 17)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(yki3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eodev22(values,b):\n",
    "    ''' sanse: binning version define bin sizes, orders data and puts it in the bins'''\n",
    "    orders=np.sort(values, axis=0)\n",
    "    a=np.shape(values)[0]\n",
    "    incv=np.floor(a/b)\n",
    "    #ardf=np.zeros(np.shape(values))\n",
    "    ep=[]\n",
    "    for i in range(1,b):\n",
    "        \n",
    "        ep.append(orders[int(incv*(i)),:])\n",
    "        \n",
    "    ep=np.asarray(ep)\n",
    "    del values\n",
    "    del orders\n",
    "    #del ardf\n",
    "    return (ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def badmathflat2(values, ep, b):\n",
    "''' sanse: applies thresholds'''\n",
    "\n",
    "    ardf=np.zeros(np.shape(values))    \n",
    "    ardf[(values[:,:]<ep[0,:]).nonzero()]=0\n",
    "    ardf[(values[:,:]>=ep[b-2,:]).nonzero()]=b-1\n",
    "\n",
    "    for i in range(1,b-1):\n",
    "        ardf[((values[:,:]>=ep[i-1,:]) & (values[:,:]<ep[i,:])).nonzero()]=i\n",
    "    del values\n",
    "    #del orders\n",
    "    #del ardf\n",
    "    return (ardf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eodev(values,b):\n",
    "    orders=np.sort(values, axis=0)\n",
    "    a=np.shape(values)[0]\n",
    "    incv=np.floor(a/b)\n",
    "    #ardf=np.zeros(np.shape(values))\n",
    "    ep=[]\n",
    "    for i in range(0,b):\n",
    "        if i<b-1:\n",
    "            ep.append(orders[int(incv*(i+1)),:])\n",
    "        else:\n",
    "            ep.append(orders[a-1,:])\n",
    "    ep=np.asarray(ep)\n",
    "    del values\n",
    "    del orders\n",
    "    #del ardf\n",
    "    return (ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def badmathflat(values, ep, b):\n",
    "    ardf=np.zeros(np.shape(values))\n",
    "    #ep=[]\n",
    "    for x in range(0,17):\n",
    "        for i in range(0,b):\n",
    "            if i==0:\n",
    "                ardf[(values[:,x]<ep[i,x]).nonzero(),x]=i\n",
    "            elif i==b-1:\n",
    "                ardf[((values[:,x]>=ep[i-1,x]) ).nonzero(),x]=i\n",
    "            else:\n",
    "                ardf[((values[:,x]>=ep[i-1,x]) & (values[:,x]<ep[i,x])).nonzero(),x]=i\n",
    "    del values\n",
    "    #del orders\n",
    "    #del ardf\n",
    "    return (ardf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 17)\n"
     ]
    }
   ],
   "source": [
    "ep = eodev(yki3,64) #swap b=8 with 12 when done testing out 8\n",
    "print(np.shape(ep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevent Code starts Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for fractal features \n",
    "## Minkowski (O.G.) box-counting\n",
    "'''sanse: not sure if it is a good thing or not, does not look hugely valuable.'''\n",
    "'''sanse: open-source repository (public), Omar's project.'''\n",
    "'''sanse: Check. Combine them with better masks.'''\n",
    "def Minkowski_FD(Z, threshold=0.9):\n",
    "    # Only for 2d image\n",
    "    assert(len(Z.shape) == 2)\n",
    "\n",
    "    # From https://github.com/rougier/numpy-100 (#87)\n",
    "    \n",
    "    def boxcount(Z, k):\n",
    "        S = np.add.reduceat(\n",
    "            np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),\n",
    "                               np.arange(0, Z.shape[1], k), axis=1)\n",
    "\n",
    "        # We count non-empty (0) and non-full boxes (k*k)\n",
    "        return len(np.where((S > 0) & (S < k*k))[0])\n",
    "\n",
    "    # Transform Z into a binary array\n",
    "    Z = (Z < np.nanmean(Z))#threshold)\n",
    "\n",
    "    # Minimal dimension of image\n",
    "    p = min(Z.shape)\n",
    "\n",
    "    # Greatest power of 2 less than or equal to p\n",
    "    n = 2**np.floor(np.log(p)/np.log(2))\n",
    "\n",
    "    # Extract the exponent\n",
    "    n = int(np.log(n)/np.log(2))\n",
    "\n",
    "    # Build successive box sizes (from 2**n down to 2**1)\n",
    "    sizes = 2**np.arange(n, 1, -1)\n",
    "\n",
    "    # Actual box counting with decreasing size\n",
    "    counts = []\n",
    "    for size in sizes:\n",
    "        counts.append(boxcount(Z, size))\n",
    "\n",
    "    # Fit the successive log(sizes) with log (counts)\n",
    "    coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)\n",
    "    return -coeffs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hfda\n",
    "import hfda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyradiomics\n",
      "  Downloading pyradiomics-3.0.1-cp37-cp37m-manylinux1_x86_64.whl (188 kB)\n",
      "\u001b[K     || 188 kB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.2 in /opt/conda/lib/python3.7/site-packages (from pyradiomics) (1.18.4)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from pyradiomics) (1.1.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from pyradiomics) (1.15.0)\n",
      "Collecting pykwalify>=1.6.0\n",
      "  Downloading pykwalify-1.8.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from pykwalify>=1.6.0->pyradiomics) (2.8.1)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /opt/conda/lib/python3.7/site-packages (from pykwalify>=1.6.0->pyradiomics) (0.6.2)\n",
      "Collecting ruamel.yaml>=0.16.0\n",
      "  Downloading ruamel.yaml-0.16.12-py2.py3-none-any.whl (111 kB)\n",
      "\u001b[K     || 111 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ruamel.yaml.clib>=0.1.2\n",
      "  Downloading ruamel.yaml.clib-0.2.2-cp37-cp37m-manylinux1_x86_64.whl (547 kB)\n",
      "\u001b[K     || 547 kB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting SimpleITK>=0.9.1\n",
      "  Downloading SimpleITK-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (47.4 MB)\n",
      "\u001b[K     || 47.4 MB 32 kB/s s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel.yaml, SimpleITK, pykwalify, pyradiomics\n",
      "  Attempting uninstall: ruamel.yaml\n",
      "    Found existing installation: ruamel-yaml 0.15.80\n",
      "    Uninstalling ruamel-yaml-0.15.80:\n",
      "      Successfully uninstalled ruamel-yaml-0.15.80\n",
      "Successfully installed SimpleITK-2.0.2 pykwalify-1.8.0 pyradiomics-3.0.1 ruamel.yaml-0.16.12 ruamel.yaml.clib-0.2.2\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyradiomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hfda\n",
    "from radiomics import firstorder, glcm, imageoperations, shape, glrlm, glszm, getTestCase, ngtdm\n",
    "from radiomics import featureextractor\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for pyradiomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eodev22(values,b):\n",
    "    '''sos: actual version of binning'''\n",
    "    orders=np.sort(values, axis=0)\n",
    "    a=np.shape(values)[0]\n",
    "    incv=np.floor(a/b)\n",
    "    #ardf=np.zeros(np.shape(values))\n",
    "    ep=[]\n",
    "    for i in range(1,b):\n",
    "        \n",
    "        ep.append(orders[int(incv*(i)),:])\n",
    "        \n",
    "    ep=np.asarray(ep)\n",
    "    del values\n",
    "    del orders\n",
    "    #del ardf\n",
    "    return (ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def badmathflat2(values, ep, b):\n",
    "    ardf=np.zeros(np.shape(values))    \n",
    "    ardf[(values[:,:]<ep[0,:]).nonzero()]=0\n",
    "    ardf[(values[:,:]>=ep[b-2,:]).nonzero()]=b-1\n",
    "\n",
    "    for i in range(1,b-1):\n",
    "        ardf[((values[:,:]>=ep[i-1,:]) & (values[:,:]<ep[i,:])).nonzero()]=i\n",
    "    del values\n",
    "    #del orders\n",
    "    #del ardf\n",
    "    return (ardf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probeStruct': {'name': 'C7-3/50', 'type': 0, 'transmitoffset': 0, 'vendors': '\\n\\t\\t\\t\\t\\n          \\n\\t\\t\\t\\t', 'biopsy': '\\n\\t\\t\\t\\n        \\n\\t\\t\\t', 'maxfocusdistance': 300000, 'maxsteerangle': 15000, 'minFocusDistanceDoppler': 80000, 'minlineduration': 120, 'radius': 50420, 'numElements': 128, 'pinOffset': 0, 'pitch': 479, 'support': '\\n\\t\\t\\t\\n        \\n\\t\\t\\t', 'muxWrap': '\\n\\t\\t\\t\\n        \\n\\t\\t\\t', 'elevationLength': 1, 'maxPwPrp': 800, 'invertedElements': 0, 'frequency': {'center': 5000000, 'bandwidth': 4000000}, 'motor': {'FOV': 0, 'homeMethod': 0, 'minTimeBetweenPulses': 0, 'motor_radius': 0, 'steps': 0, 'homeCorrection': 0}}, 'studyMode': 'RF', 'file': 'lev1bad.pkl', 'filepath': '', 'probe': 'clarius', 'system': 'Clarius', 'studyID': 'ev1bad', 'samples': 1831759474, 'lines': 1634036843, 'depthOffset': 0, 'depth': 17692376210, 'width': 230, 'rxFrequency': 20000000, 'samplingFrequency': 20000000, 'txFrequency': 4, 'centerFrequency': 4, 'targetFOV': 0, 'numFocalZones': 1, 'numFrames': 1935878144, 'frameSize': 4069246528300, 'depthAxis': 17692376210, 'widthhAxis': 230, 'lineDensity': 192, 'height': 17692376210, 'pitch': 479, 'dynRange': 0, 'yOffset': 0, 'vOffset': 0, 'lowBandFreq': -1999996.0, 'upBandFreq': 2000004.0, 'gain': 0, 'rxGain': 0, 'userGain': 0, 'txPower': 0, 'power': 0, 'PRF': 0, 'yRes': 0.0385, 'yResRF': 9.658678697244724, 'xRes': 0.3193333333333333, 'xResRF': 1.4075570020669355e-07, 'quad2X': 1, 'Apitch': 3.85e-05, 'Lpitch': 0.0003193333333333333, 'Radius': 0.05042, 'PixelsPerMM': 8, 'lateralRes': 0.125, 'axialRes': 0.125}\n",
      "{'probeStruct': {'name': 'C7-3/50', 'type': 0, 'transmitoffset': 0, 'vendors': '\\n\\t\\t\\t\\t\\n          \\n\\t\\t\\t\\t', 'biopsy': '\\n\\t\\t\\t\\n        \\n\\t\\t\\t', 'maxfocusdistance': 300000, 'maxsteerangle': 15000, 'minFocusDistanceDoppler': 80000, 'minlineduration': 120, 'radius': 50420, 'numElements': 128, 'pinOffset': 0, 'pitch': 479, 'support': '\\n\\t\\t\\t\\n        \\n\\t\\t\\t', 'muxWrap': '\\n\\t\\t\\t\\n        \\n\\t\\t\\t', 'elevationLength': 1, 'maxPwPrp': 800, 'invertedElements': 0, 'frequency': {'center': 5000000, 'bandwidth': 4000000}, 'motor': {'FOV': 0, 'homeMethod': 0, 'minTimeBetweenPulses': 0, 'motor_radius': 0, 'steps': 0, 'homeCorrection': 0}}, 'studyMode': 'RF', 'file': 'lev1bad.pkl', 'filepath': '', 'probe': 'clarius', 'system': 'Clarius', 'studyID': 'ev1bad', 'samples': 1831759474, 'lines': 1634036843, 'depthOffset': 0, 'depth': 17692376210, 'width': 230, 'rxFrequency': 20000000, 'samplingFrequency': 20000000, 'txFrequency': 4, 'centerFrequency': 4, 'targetFOV': 0, 'numFocalZones': 1, 'numFrames': 1935878144, 'frameSize': 4069246528300, 'depthAxis': 17692376210, 'widthhAxis': 230, 'lineDensity': 192, 'height': 17692376210, 'pitch': 479, 'dynRange': 0, 'yOffset': 0, 'vOffset': 0, 'lowBandFreq': -1999996.0, 'upBandFreq': 2000004.0, 'gain': 0, 'rxGain': 0, 'userGain': 0, 'txPower': 0, 'power': 0, 'PRF': 0, 'yRes': 0.0385, 'yResRF': 9.658678697244724, 'xRes': 0.3193333333333333, 'xResRF': 1.4075570020669355e-07, 'quad2X': 1, 'Apitch': 3.85e-05, 'Lpitch': 0.0003193333333333333, 'Radius': 0.012605, 'PixelsPerMM': 2, 'lateralRes': 0.125, 'axialRes': 0.125}\n"
     ]
    }
   ],
   "source": [
    "## For scan-conversion\n",
    "with open('Iminf_.pkl', 'rb') as handle:\n",
    "    imgInfo = pickle.load(handle)\n",
    "print(imgInfo)\n",
    "imgInfo2=copy.deepcopy(imgInfo)\n",
    "# Run only ONCE! Adjusts imgInfo to match the downsampling of the QUS heatmaps\n",
    "imgInfo2[\"PixelsPerMM\"]=2\n",
    "imgInfo2[\"Radius\"]=imgInfo2[\"Radius\"]/4\n",
    "print(imgInfo)\n",
    "'''sanse: two versions, since sth. else is scan-converted'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are for the sinple SP Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph1=np.load('PhantforNak.npy')\n",
    "'''sms: load up phantom'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2928, 192)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(ph1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NakaGamiParam2(c,v):\n",
    "'''sms: it calculates again NP2 (correct some mistakes in QUS map), and adds some additional stuff (snr). We stack both NPs into the network'''\n",
    "    from scipy.signal import hilbert\n",
    "    #c is patient rf data, v is phantom rf data\n",
    "    r=np.abs(hilbert(c,axis=1))\n",
    "#     r=hilbert(c,axis=1)\n",
    "    #print(np.shape(r))\n",
    "    #p=hilbert(v,axis=1)\n",
    "    p=np.abs(hilbert(v,axis=1))\n",
    "    #w=np.nanmean((r)**2)\n",
    "    #u=((np.nanmean((r)**2))**2)/(np.nanmean((((r)**2)-np.nanmean((r)**2))**2))\n",
    "    w=np.nanmean((r/p)**2,axis=1)\n",
    "    #print(np.shape(w))\n",
    "    u=((np.nanmean((r/p)**2,axis=1))**2)/np.var((r/p)**2,axis=1)#(np.nanmean((((r/p)**2)-np.nanmean((r/p)**2,axis=1).reshape(-1,1,1))**2,axis=1))\n",
    "    \n",
    "    #print(np.shape(u))\n",
    "    w=np.nanmean(w,axis=1)\n",
    "    u=np.nanmean(u,axis=1)\n",
    "    srm=np.nanmean(r,axis=1)\n",
    "    srsd=np.nanstd(r,axis=1)\n",
    "    srp=srm/srsd\n",
    "    sr=np.nanmean(srp,axis=1)\n",
    "    \n",
    "    #return[np.abs(w),np.abs(u),np.abs(sr)]\n",
    "    return[w,u,sr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setforrad3(trial):\n",
    "'''sanse: discretize dataset for entropy calculations. Discretization can be improved (check)'''\n",
    "'''sanse: heuristic method'''\n",
    "'''sanse: threshold values tuned graphically'''\n",
    "    ar=np.zeros(np.shape(trial))#initialize a variable for holding new values\n",
    "    \n",
    "    #Read up on quantization\n",
    "    #non linear homebrew method\n",
    "    (a,b)=(trial<=-3000).nonzero()\n",
    "    #at=np.floor((trial-np.min(trial))/(-3000-np.min(trial))*25)\n",
    "    ar[a,b]=np.floor(((trial[a,b]-np.min(trial))/(-3000-np.min(trial)))*25)#at[a,b]\n",
    "\n",
    "    #at=(np.floor(((trial-(-3000))/(-1000-(-3000)))*25)+25)\n",
    "\n",
    "    (a,b)=((-3000<trial) & (trial<=-1000)).nonzero()\n",
    "    ar[a,b]=(np.floor(((trial[a,b]-(-3000))/(-1000-(-3000)))*25)+25)#at[a,b]\n",
    "\n",
    "\n",
    "    #at=(np.floor(((trial-(-1000))/(-500-(-1000)))*25)+50)\n",
    "    (a,b)=((-1000<trial) & (trial<=-500)).nonzero()\n",
    "    ar[a,b]=(np.floor(((trial[a,b]-(-1000))/(-500-(-1000)))*25)+50)#at[a,b]\n",
    "\n",
    "\n",
    "    #at=(np.floor(((trial-(-500))/(500-(-500)))*100)+75)\n",
    "    (a,b)=((-500<trial) & (trial<=500)).nonzero()\n",
    "\n",
    "    ar[a,b]=(np.floor(((trial[a,b]-(-500))/(500-(-500)))*100)+75)#at[a,b]\n",
    "\n",
    "\n",
    "    #at=(np.floor(((trial-(500))/(1000-(500)))*25)+175)\n",
    "    (a,b)=((500<trial) & (trial<=1000)).nonzero()\n",
    "    ar[a,b]=(np.floor(((trial[a,b]-(500))/(1000-(500)))*25)+175)#at[a,b]\n",
    "\n",
    "\n",
    "\n",
    "    #at=(np.floor(((trial-(1000))/(3000-(1000)))*25)+200)\n",
    "    (a,b)=((1000<trial) & (trial<=3000)).nonzero()\n",
    "    ar[a,b]=(np.floor(((trial[a,b]-(1000))/(3000-(1000)))*25)+200)#at[a,b]\n",
    "\n",
    "    #at=(np.floor(((trial-(3000))/(np.max(trial)-(3000)))*25)+225)\n",
    "    (a,b)=(3000<trial).nonzero()\n",
    "    ar[a,b]=(np.floor(((trial[a,b]-(3000))/(np.max(trial)-(3000)))*25)+225)#at[a,b]\n",
    "    \n",
    "    #r=ar[:,:,0]\n",
    "    #ab=np.ones(np.shape(r))\n",
    "    \n",
    "    return (ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import hilbert\n",
    "from scipy.stats import skew, entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are for Bmode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import uniform_filter, median_filter\n",
    "from scipy.ndimage.measurements import variance\n",
    "    ''' sanse: we-save B-modes, which potentially will be used in neural nets (new project)'''\n",
    "def lee_filter(img, size):\n",
    "    img_mean = uniform_filter(img, (size[0], size[1]))\n",
    "    img_sqr_mean = uniform_filter(img**2, (size[0], size[1]))\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = (img_variance / (img_variance + overall_variance))\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "    return img_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanconvert2(Iin=None, info=None):#Apitch, Lpitch, Radius, PixelsPerMM)\n",
    "    ''' sanse: faster version of scan conversion, not deployed in MaskingCleaned (check on order of indices, it changes)'''\n",
    "    ''' sanse: could be updated in MaskingCleaned (check)'''\n",
    "    \n",
    "    import numpy as np\n",
    "    if (info[\"Radius\"] == 0):    # A linear array probe\n",
    "        print(\"Linear Array\")\n",
    "        AxialExtent = info[\"Apitch\"] * np.size(Iin, 0)\n",
    "        LateralExtent = info[\"Lpitch\"] * np.size(Iin, 1)\n",
    "        AxialSize = AxialExtent * 1000 * info[\"PixelsPerMM\"]\n",
    "        LateralSize = LateralExtent * 1000 * info[\"PixelsPerMM\"]\n",
    "        Iout = np.resize(Iin, (LateralSize, AxialSize))    # cubic interpolation\n",
    "    else:    # A curved array probe    \n",
    "        t = ((np.arange(1,Iin.shape[2],dtype=int)) - Iin.shape[2]/ 2) * info[\"Lpitch\"] / info[\"Radius\"]\n",
    "        r = info[\"Radius\"] + (np.arange(1,Iin.shape[1])) * info[\"Apitch\"]\n",
    "        [t, r] = np.meshgrid(t, r)\n",
    "        x = np.multiply(r, np.cos(t))\n",
    "        y = np.multiply(r, np.sin(t))\n",
    "    \n",
    "        divides=1e-3 / info[\"PixelsPerMM\"]\n",
    "        xMin=np.min(x)\n",
    "        xMax=np.max(x)\n",
    "        #print(\"X Min : %s , X Max %s\" % (xMin,xMax))\n",
    "        #print(\"Divides : %s \" % divides)\n",
    "        xreg =  np.arange(np.min(x) , np.max(x) ,divides)\n",
    "        yreg =  np.arange(np.min(y) , np.max(y) ,divides)\n",
    "        [yreg, xreg] = np.meshgrid(yreg, xreg) \n",
    "        temp=np.hstack((np.shape(Iin)[0],xreg.shape))\n",
    "        Iout = np.zeros(temp)#xreg.shape)\n",
    "        xCntr = np.arange(0,xreg.shape[1])\n",
    "        Cntry = np.arange(0,yreg.shape[0])\n",
    "        theta = np.array([cart4pol(xreg[yCntr, xCntr], yreg[yCntr, xCntr]) for yCntr in Cntry])\n",
    "        rho = np.array([cart2pol(xreg[yCntr, xCntr], yreg[yCntr, xCntr]) for yCntr in Cntry])\n",
    "        #for xCntr in tqdm(np.arange(0,xreg.shape[1])):\n",
    "            #for yCntr in np.arange(0,yreg.shape[0]):\n",
    "                #[theta, rho] = cart2pol(xreg[yCntr, xCntr], yreg[yCntr, xCntr])\n",
    "                #print(\"theta : %s , rho %s\" % (theta,rho))\n",
    "        indt = np.floor(theta / (info[\"Lpitch\"] / info[\"Radius\"]) + Iin.shape[2] / 2) \n",
    "        indr = np.floor((rho - info[\"Radius\"]) / info[\"Apitch\"]) \n",
    "\n",
    "        [j,i]=((indt>=0)&(indt <= t.shape[1])& (indr>= 0) &(indr <= r.shape[0])).nonzero()\n",
    "        #print(np.size(j))\n",
    "        j=j.astype(int)\n",
    "        i=i.astype(int)\n",
    "        Iout[:,j, i] = Iin[:,indr[j,i].astype(int), indt[j,i].astype(int)]\n",
    "        #for i in xCntr:\n",
    "            \n",
    "         #   for j in Cntry:\n",
    "                \n",
    "          #      if indt[j,i] >= 0 and (indt[j,i] <= t.shape[1]and (indr[j,i] >= 0 and indr[j,i] <= r.shape[0])):\n",
    "                    \n",
    "           #         Iout[int(Cntry[j]), int(xCntr[i])] = Iin[int(indr[j,i]), int(indt[j,i])]\n",
    "    #print(np.shape(Iout))\n",
    "    return Iout\n",
    "\n",
    "def cart2pol(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    return rho\n",
    "def cart4pol(x, y):\n",
    "    phi = np.arctan2(y, x)\n",
    "    return phi\n",
    "\n",
    "\n",
    "def pol2cart(rho, phi):\n",
    "    x = rho * np.cos(phi)\n",
    "    y = rho * np.sin(phi)\n",
    "    return(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import hilbert\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HEY ITS THIS ONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Add here new cell from adi'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:29: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c01ad7d6c9c465da1f4f1431d8efcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in less\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 29min 23s, sys: 52 s, total: 30min 15s\n",
      "Wall time: 9min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainvidtopat_h0, trainvid_h0, trainlabvid_h0, trainframetovid_h0, trainframe_h0, trainwind_h0,trainlabframe_h0, trainweightrfh0,trainweightqnth0,windtoframeh0 = load_qus_textft(h3,h3k[0][5:8],0,h3kpa,h3cap,ep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.hstack((trainweightqnth0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trainframetovid_h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:29: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f91fdb849bd4b07ae8331ebced3c04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in less\n",
      "  app.launch_new_instance()\n",
      "/opt/conda/lib/python3.7/site-packages/hfda/core.py:67: RuntimeWarning: divide by zero encountered in log2\n",
      "  D, _ = - np.polyfit(np.log2(k), np.log2(L), 1)\n",
      "/opt/conda/lib/python3.7/site-packages/hfda/core.py:12: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  norm = (N-1) / (n*k)\n",
      "/opt/conda/lib/python3.7/site-packages/hfda/core.py:12: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  norm = (N-1) / (n*k)\n",
      "/opt/conda/lib/python3.7/site-packages/hfda/core.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  Lm = (sum*norm) / k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 5min 49s, sys: 16.8 s, total: 6min 5s\n",
      "Wall time: 6min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainvidtopat_h0, trainvid_h0, trainlabvid_h0, trainframetovid_h0, trainframe_h0, trainwind_h0,trainlabframe_h0, trainweightrfh0,trainweightqnth0,windtoframeh0 = load_qus_textft(h3,h3k[0][0:2],0,h3kpa,h3cap,ep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "merg=np.empty([204,476])\n",
    "for i in range(0,204):\n",
    "    merg[i,:]=trainframe_h0[i][-476:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.33719479,  9.81720534, 10.26744977, 10.33130126, 10.57594835,\n",
       "       10.82799624, 10.93369941, 11.19675351, 11.29763137, 11.3750424 ,\n",
       "       11.45497232, 11.61073424, 11.7376362 , 11.79587503, 11.85424859,\n",
       "       11.89540029, 11.90639763, 11.96253867, 11.99953923, 12.09351194,\n",
       "       12.12113148, 12.13703822, 12.16762823, 12.22195895, 12.41446848,\n",
       "       12.46786665, 12.66943639, 12.70196719, 12.71801627, 12.85731524,\n",
       "       12.96462994, 12.9692167 , 12.98949348, 13.02753159, 13.12185371,\n",
       "       13.26199741, 13.27635832, 13.35431483, 13.45333348, 13.46079853,\n",
       "       13.46236106, 13.47566758, 13.55927461, 13.56912389, 13.60679363,\n",
       "       13.63465513, 13.65780226, 13.67660495, 13.67802869, 13.68058463,\n",
       "       13.70906076, 13.74706385, 13.7765663 , 13.81584354, 13.86549071,\n",
       "       13.8755452 , 13.90704032, 13.91048792, 13.92149204, 13.92525028,\n",
       "       13.92865011, 13.9774431 , 13.98266601, 14.0611594 , 14.08601579,\n",
       "       14.08859298, 14.09805075, 14.10566954, 14.10817613, 14.11048649,\n",
       "       14.11769498, 14.12187557, 14.14741791, 14.18189312, 14.20653146,\n",
       "       14.21081079, 14.21951062, 14.24302134, 14.25600275, 14.27513388,\n",
       "       14.28835188, 14.32592731, 14.33499909, 14.35253856, 14.35507972,\n",
       "       14.3654921 , 14.3824664 , 14.41128797, 14.41812716, 14.43302156,\n",
       "       14.44327281, 14.47314647, 14.49416224, 14.49426984, 14.51400177,\n",
       "       14.52097912, 14.52407996, 14.53166213, 14.57055274, 14.60978347,\n",
       "       14.62256743, 14.63836644, 14.64678389, 14.65033495, 14.66913359,\n",
       "       14.67246552, 14.67459224, 14.67557354, 14.71102032, 14.7137807 ,\n",
       "       14.73009728, 14.73830401, 14.74042555, 14.75569333, 14.76263977,\n",
       "       14.77274046, 14.84746169, 14.85985096, 14.86919103, 14.88594747,\n",
       "       14.90413406, 14.90585123, 14.90712163, 14.90993921, 14.92375369,\n",
       "       14.92510802, 14.92633257, 14.94037158, 14.94207575, 15.00492145,\n",
       "       15.02596941, 15.04089719, 15.04679009, 15.11775853, 15.11813906,\n",
       "       15.12539743, 15.17371304, 15.18478988, 15.19905646, 15.20308072,\n",
       "       15.22551708, 15.23347319, 15.24136234, 15.25283711, 15.30001218,\n",
       "       15.3078534 , 15.37327486, 15.37747613, 15.38095434, 15.40073396,\n",
       "       15.40885622, 15.43678448, 15.43869799, 15.44432259, 15.49388417,\n",
       "       15.49594566, 15.49841505, 15.52492494, 15.52771713, 15.59831817,\n",
       "       15.62590472, 15.76775266, 15.76951408, 15.79030545, 15.8058855 ,\n",
       "       15.8085122 , 15.82734876, 15.88418396, 15.93514703, 15.93671916,\n",
       "       15.97148058, 16.00332019, 16.01264742, 16.03945528, 16.06057926,\n",
       "       16.06211931, 16.11413396, 16.12976819, 16.14102096, 16.16859347,\n",
       "       16.23701653, 16.25180684, 16.257356  , 16.26464703, 16.28514452,\n",
       "       16.28782753, 16.34212532, 16.44373226, 16.52579171, 16.56851573,\n",
       "       16.59124915, 16.61475954, 16.6262445 , 16.64278306, 16.78288708,\n",
       "       16.86516686, 16.87334197, 16.88587457, 17.01970583, 17.03426997,\n",
       "       17.24309798, 17.29629099, 17.32911502, 17.9132312 ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(merg[:,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:29: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990b7cde364d490da18fc3f8fba9c20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=58.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in less\n",
      "  app.launch_new_instance()\n",
      "/opt/conda/lib/python3.7/site-packages/hfda/core.py:67: RuntimeWarning: divide by zero encountered in log2\n",
      "  D, _ = - np.polyfit(np.log2(k), np.log2(L), 1)\n",
      "/opt/conda/lib/python3.7/site-packages/hfda/core.py:12: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  norm = (N-1) / (n*k)\n",
      "/opt/conda/lib/python3.7/site-packages/hfda/core.py:12: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  norm = (N-1) / (n*k)\n",
      "/opt/conda/lib/python3.7/site-packages/hfda/core.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  Lm = (sum*norm) / k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 24)\n",
      "(17, 4)\n",
      "(408,)\n",
      "(68,)\n"
     ]
    }
   ],
   "source": [
    "trainvidtopat_c0, trainvid_c0, trainlabvid_c0, trainframetovid_c0, trainframe_c0, trainwind_c0, trainlabframe_c0,trainweightrfc0,trainweightqntc0,windtoframec0 = load_qus_textft(c3,c3k,3,c3kpa,c3cap,ep)#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:29: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59badd13d88490cb3d4890c15e874d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=47.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in less\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "trainvidtopat_h0, trainvid_h0, trainlabvid_h0, trainframetovid_h0, trainframe_h0, trainwind_h0,trainlabframe_h0, trainweightrfh0,trainweightqnth0,windtoframeh0 ,trainwinloch0,trainstatloc_h0, trainsimstats_h0, trainpowerspec_h0,trainBmodeh0   = load_qus_textft(h3,h3k[0],0,h3kpa,h3cap,ep)\n",
    "trainvidtopat_f20, trainvid_f20, trainlabvid_f20, trainframetovid_f20, trainframe_f20, trainwind_f20, trainlabframe_f20,trainweightrff20,trainweightqntf20,windtoframef20,trainwinlocf20,trainstatloc_f20, trainsimstats_f20, trainpowerspec_f20,trainBmodef20= load_qus_textft(f23,f2k[0],1,f2kpa,f2cap,ep)\n",
    "trainvidtopat_f30, trainvid_f30, trainlabvid_f30, trainframetovid_f30, trainframe_f30, trainwind_f30, trainlabframe_f30,trainweightrff30,trainweightqntf30,windtoframef30, trainwinlocf30,trainstatloc_f30, trainsimstats_f30, trainpowerspec_f30,trainBmodef30= load_qus_textft(f33,f3k[0],2,f3kpa,f3cap,ep)\n",
    "trainvidtopat_c0, trainvid_c0, trainlabvid_c0, trainframetovid_c0, trainframe_c0, trainwind_c0, trainlabframe_c0,trainweightrfc0,trainweightqntc0,windtoframec0,trainwinlocc0,trainstatloc_c0, trainsimstats_c0, trainpowerspec_c0 ,trainBmodec0   = load_qus_textft(c3,c3k[0],3,c3kpa,c3cap,ep)#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvidtopat_h1, trainvid_h1, trainlabvid_h1, trainframetovid_h1, trainframe_h1, trainwind_h1,trainlabframe_h1, trainweightrfh1,trainweightqnth1,windtoframeh1 ,trainwinloch1,trainstatloc_h1, trainsimstats_h1, trainpowerspec_h1  ,trainBmodeh1 = load_qus_textft(h3,h3k[1],0,h3kpa,h3cap,ep)\n",
    "trainvidtopat_f21, trainvid_f21, trainlabvid_f21, trainframetovid_f21, trainframe_f21, trainwind_f21, trainlabframe_f21,trainweightrff21,trainweightqntf21,windtoframef21,trainwinlocf21,trainstatloc_f21, trainsimstats_f21, trainpowerspec_f21,trainBmodef21= load_qus_textft(f23,f2k[1],1,f2kpa,f2cap,ep)\n",
    "trainvidtopat_f31, trainvid_f31, trainlabvid_f31, trainframetovid_f31, trainframe_f31, trainwind_f31, trainlabframe_f31,trainweightrff31,trainweightqntf31,windtoframef31,trainwinlocf31,trainstatloc_f31, trainsimstats_f31, trainpowerspec_f31,trainBmodef31 = load_qus_textft(f33,f3k[1],2,f3kpa,f3cap,ep)\n",
    "trainvidtopat_c1, trainvid_c1, trainlabvid_c1, trainframetovid_c1, trainframe_c1, trainwind_c1, trainlabframe_c1,trainweightrfc1,trainweightqntc1,windtoframec1,trainwinlocc1,trainstatloc_c1, trainsimstats_c1, trainpowerspec_c1 ,trainBmodec1  = load_qus_textft(c3,c3k[1],3,c3kpa,c3cap,ep)#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvidtopat_h2, trainvid_h2, trainlabvid_h2, trainframetovid_h2, trainframe_h2, trainwind_h2,trainlabframe_h2, trainweightrfh2,trainweightqnth2,windtoframeh2,trainwinloch2 ,trainstatloc_h2, trainsimstats_h2, trainpowerspec_h2   ,trainBmodeh2 = load_qus_textft(h3,h3k[2],0,h3kpa,h3cap,ep)\n",
    "trainvidtopat_f22, trainvid_f22, trainlabvid_f22, trainframetovid_f22, trainframe_f22, trainwind_f22, trainlabframe_f22,trainweightrff22,trainweightqntf22,windtoframef22,trainwinlocf22,trainstatloc_f22, trainsimstats_f22, trainpowerspec_f22,trainBmodef22= load_qus_textft(f23,f2k[2],1,f2kpa,f2cap,ep)\n",
    "trainvidtopat_f32, trainvid_f32, trainlabvid_f32, trainframetovid_f32, trainframe_f32, trainwind_f32, trainlabframe_f32,trainweightrff32,trainweightqntf32,windtoframef32,trainwinlocf32,trainstatloc_f32, trainsimstats_f32, trainpowerspec_f32,trainBmodef32 = load_qus_textft(f33,f3k[2],2,f3kpa,f3cap,ep)\n",
    "trainvidtopat_c2, trainvid_c2, trainlabvid_c2, trainframetovid_c2, trainframe_c2, trainwind_c2, trainlabframe_c2,trainweightrfc2,trainweightqntc2,windtoframec2,trainwinlocc2 ,trainstatloc_c2, trainsimstats_c2, trainpowerspec_c2 ,trainBmodec2  = load_qus_textft(c3,c3k[2],3,c3kpa,c3cap,ep)#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvidtopat_h3, trainvid_h3, trainlabvid_h3, trainframetovid_h3, trainframe_h3, trainwind_h3,trainlabframe_h3, trainweightrfh3,trainweightqnth3 ,windtoframeh3,trainwinloch3,trainstatloc_h3, trainsimstats_h3, trainpowerspec_h3  trainBmodeh3 = load_qus_textft(h3,h3k[3],0,h3kpa,h3cap,ep)\n",
    "trainvidtopat_f23, trainvid_f23, trainlabvid_f23, trainframetovid_f23, trainframe_f23, trainwind_f23, trainlabframe_f23,trainweightrff23,trainweightqntf23,windtoframef23,trainwinlocf23,trainstatloc_f23, trainsimstats_f23, trainpowerspec_f23trainBmodef23= load_qus_textft(f23,f2k[3],1,f2kpa,f2cap,ep)\n",
    "trainvidtopat_f33, trainvid_f33, trainlabvid_f33, trainframetovid_f33, trainframe_f33, trainwind_f33, trainlabframe_f33,trainweightrff33,trainweightqntf33,windtoframef33,trainwinlocf33,trainstatloc_f33, trainsimstats_f33, trainpowerspec_f33trainBmodef33 = load_qus_textft(f33,f3k[3],2,f3kpa,f3cap,ep)\n",
    "trainvidtopat_c3, trainvid_c3, trainlabvid_c3, trainframetovid_c3, trainframe_c3, trainwind_c3, trainlabframe_c3,trainweightrfc3,trainweightqntc3,windtoframec3,trainwinlocc3  ,trainstatloc_c3, trainsimstats_c3, trainpowerspec_c3 trainBmodec3  = load_qus_textft(c3,c3k[3],3,c3kpa,c3cap,ep)#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvidtopat_h4, trainvid_h4, trainlabvid_h4, trainframetovid_h4, trainframe_h4, trainwind_h4,trainlabframe_h4, trainweightrfh4,trainweightqnth4 ,windtoframeh4,trainwinloch4,trainstatloc_h4, trainsimstats_h4, trainpowerspec_h4 , trainBmodeh4 = load_qus_textft(h3,h3k[4],0,h3kpa,h3cap,ep)\n",
    "trainvidtopat_f24, trainvid_f24, trainlabvid_f24, trainframetovid_f24, trainframe_f24, trainwind_f24, trainlabframe_f24,trainweightrff24,trainweightqntf24,windtoframef24,trainwinlocf24,trainstatloc_f24, trainsimstats_f24, trainpowerspec_f24,trainBmodef24= load_qus_textft(f23,f2k[4],1,f2kpa,f2cap,ep)\n",
    "trainvidtopat_f34, trainvid_f34, trainlabvid_f34, trainframetovid_f34, trainframe_f34, trainwind_f34, trainlabframe_f34,trainweightrff34,trainweightqntf34,windtoframef34,trainwinlocf34,trainstatloc_f34, trainsimstats_f34, trainpowerspec_f34,trainBmodef34 = load_qus_textft(f33,f3k[4],2,f3kpa,f3cap,ep)\n",
    "trainvidtopat_c4, trainvid_c4, trainlabvid_c4, trainframetovid_c4, trainframe_c4, trainwind_c4, trainlabframe_c4,trainweightrfc4,trainweightqntc4,windtoframec4,trainwinlocc4,trainstatloc_c4, trainsimstats_c4, trainpowerspec_c4 , trainBmodec4 = load_qus_textft(c3,c3k[4],3,c3kpa,c3cap,ep)#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 3, 4, 7, 4, 6, 1, 8, 7, 4, 4, 3, 8, 4]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainvidtopat_c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##run this one\n",
    "\n",
    "trainvidtopat1 = np.concatenate((trainvidtopat_h1,trainvidtopat_f21,trainvidtopat_f31,trainvidtopat_c1))#, trainvidtopat_f41))\n",
    "trainvid1 = np.concatenate((trainvid_h1, trainvid_f21,trainvid_f31, trainvid_c1))#, trainvid_f41))\n",
    "trainlabvid1 = np.concatenate((trainlabvid_h1, trainlabvid_f21,trainlabvid_f31, trainlabvid_c1))#, trainlabvid_f41))\n",
    "trainframetovid1 = np.concatenate((trainframetovid_h1,trainframetovid_f21,trainframetovid_f31,trainframetovid_c1))#,trainframetovid_f41))\n",
    "trainframe1 = np.concatenate((trainframe_h1,trainframe_f21,trainframe_f31,trainframe_c1))#,trainframe_f41))\n",
    "trainwind1 = np.concatenate((trainwind_h1,trainwind_f21,trainwind_f31,trainwind_c1))#,trainwind_f41))\n",
    "trainlabframe1 = np.concatenate((trainlabframe_h1,trainlabframe_f21,trainlabframe_f31,trainlabframe_c1))#,trainlabframe_f41))\n",
    "\n",
    "trainvidtopat2 = np.concatenate((trainvidtopat_h2,trainvidtopat_f22,trainvidtopat_f32,trainvidtopat_c2))#, trainvidtopat_f42))\n",
    "trainvid2 = np.concatenate((trainvid_h2, trainvid_f22,trainvid_f32, trainvid_c2))#, trainvid_f42))\n",
    "trainlabvid2 = np.concatenate((trainlabvid_h2,  trainlabvid_f22,trainlabvid_f32, trainlabvid_c2))#, trainlabvid_f42))\n",
    "trainframetovid2 = np.concatenate((trainframetovid_h2,trainframetovid_f22,trainframetovid_f32,trainframetovid_c2))#,trainframetovid_f42))\n",
    "trainframe2 = np.concatenate((trainframe_h2,trainframe_f22,trainframe_f32,trainframe_c2))#,trainframe_f42))\n",
    "trainwind2 = np.concatenate((trainwind_h2,trainwind_f22,trainwind_f32,trainwind_c2))#,trainwind_f42))\n",
    "trainlabframe2 = np.concatenate((trainlabframe_h2,trainlabframe_f22,trainlabframe_f32,trainlabframe_c2))#,trainlabframe_f42))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainvidtopat0 = np.concatenate((trainvidtopat_h0,trainvidtopat_f20,trainvidtopat_f30,trainvidtopat_c0))#, trainvidtopat_f41))\n",
    "trainvid0 = np.concatenate((trainvid_h0,trainvid_f20,trainvid_f30, trainvid_c0))#, trainvid_f41))\n",
    "trainlabvid0 = np.concatenate((trainlabvid_h0,  trainlabvid_f20,trainlabvid_f30, trainlabvid_c0))#, trainlabvid_f41))\n",
    "trainframetovid0 = np.concatenate((trainframetovid_h0,trainframetovid_f20,trainframetovid_f30,trainframetovid_c0))#,trainframetovid_f41))\n",
    "trainframe0 = np.concatenate((trainframe_h0,trainframe_f20,trainframe_f30,trainframe_c0))#,trainframe_f41))\n",
    "trainwind0 = np.concatenate((trainwind_h0,trainwind_f20,trainwind_f30,trainwind_c0))#,trainwind_f41))\n",
    "trainlabframe0 = np.concatenate((trainlabframe_h0,trainlabframe_f20,trainlabframe_f30,trainlabframe_c0))#,trainlabframe_f41))\n",
    "\n",
    "\n",
    "trainvidtopat3 = np.concatenate((trainvidtopat_h3,trainvidtopat_f23,trainvidtopat_f33,trainvidtopat_c3))#, trainvidtopat_f43))\n",
    "trainvid3 = np.concatenate((trainvid_h3, trainvid_f23,trainvid_f33, trainvid_c3))#, trainvid_f43))\n",
    "trainlabvid3 = np.concatenate((trainlabvid_h3, trainlabvid_f23,trainlabvid_f33, trainlabvid_c3))#, trainlabvid_f43))\n",
    "trainframetovid3 = np.concatenate((trainframetovid_h3,trainframetovid_f23,trainframetovid_f33,trainframetovid_c3))#,trainframetovid_f43))\n",
    "trainframe3 = np.concatenate((trainframe_h3,trainframe_f23,trainframe_f33,trainframe_c3))#,trainframe_f43))\n",
    "trainwind3 = np.concatenate((trainwind_h3,trainwind_f23,trainwind_f33,trainwind_c3))#,trainwind_f43))\n",
    "trainlabframe3 = np.concatenate((trainlabframe_h3,trainlabframe_f23,trainlabframe_f33,trainlabframe_c3))#,trainlabframe_f43))\n",
    "\n",
    "\n",
    "\n",
    "trainvidtopat4 = np.concatenate((trainvidtopat_h4,trainvidtopat_f24,trainvidtopat_f34,trainvidtopat_c4))#, trainvidtopat_f41))\n",
    "trainvid4 = np.concatenate((trainvid_h4, trainvid_f24,trainvid_f34, trainvid_c4))#, trainvid_f41))\n",
    "trainlabvid4 = np.concatenate((trainlabvid_h4,  trainlabvid_f24,trainlabvid_f34, trainlabvid_c4))#, trainlabvid_f41))\n",
    "trainframetovid4 = np.concatenate((trainframetovid_h4,trainframetovid_f24,trainframetovid_f34,trainframetovid_c4))#,trainframetovid_f41))\n",
    "trainframe4 = np.concatenate((trainframe_h4,trainframe_f24,trainframe_f34,trainframe_c4))#,trainframe_f41))\n",
    "trainwind4 = np.concatenate((trainwind_h4,trainwind_f24,trainwind_f34,trainwind_c4))#,trainwind_f41))\n",
    "trainlabframe4 = np.concatenate((trainlabframe_h4,trainlabframe_f24,trainlabframe_f34,trainlabframe_c4))#,trainlabframe_f41))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#run this one\n",
    "trainweightrf1 = np.concatenate((trainweightrfh1,trainweightrff21,trainweightrff31,trainweightrfc1))#,trainweightrff41))\n",
    "trainweightrf2 = np.concatenate((trainweightrfh2,trainweightrff22,trainweightrff32,trainweightrfc2))#,trainweightrff42))\n",
    "trainweightrf3 = np.concatenate((trainweightrfh3,trainweightrff23,trainweightrff33,trainweightrfc3))#,trainweightrff43))\n",
    "trainweightrf0 = np.concatenate((trainweightrfh0,trainweightrff20,trainweightrff30,trainweightrfc0))#,trainweightrff41))\n",
    "trainweightrf4 = np.concatenate((trainweightrfh4,trainweightrff24,trainweightrff34,trainweightrfc4))#,trainweightrff41))\n",
    "\n",
    "\n",
    "trainweightqnt1 = np.concatenate((trainweightqnth1,trainweightqntf21,trainweightqntf31,trainweightqntc1))#,trainweightqntf41))\n",
    "trainweightqnt2 = np.concatenate((trainweightqnth2,trainweightqntf22,trainweightqntf32,trainweightqntc2))#,trainweightqntf42))\n",
    "trainweightqnt3 = np.concatenate((trainweightqnth3,trainweightqntf23,trainweightqntf33,trainweightqntc3))#,trainweightqntf43))\n",
    "trainweightqnt0 = np.concatenate((trainweightqnth0,trainweightqntf20,trainweightqntf30,trainweightqntc0))#,trainweightqntf41))\n",
    "trainweightqnt4 = np.concatenate((trainweightqnth4,trainweightqntf24,trainweightqntf34,trainweightqntc4))#,trainweightqntf41))\n",
    "\n",
    "windtoframe1 = np.concatenate((windtoframeh1,windtoframef21,windtoframef31,windtoframec1))#,trainweightqntf41))\n",
    "windtoframe2 = np.concatenate((windtoframeh2,windtoframef22,windtoframef32,windtoframec2))#,trainweightqntf42))\n",
    "windtoframe3 = np.concatenate((windtoframeh3,windtoframef23,windtoframef33,windtoframec3))#,trainweightqntf43))\n",
    "windtoframe0 = np.concatenate((windtoframeh0,windtoframef20,windtoframef30,windtoframec0))#,trainweightqntf41))\n",
    "windtoframe4 = np.concatenate((windtoframeh4,windtoframef24,windtoframef34,windtoframec4))#,trainweightqntf41))\n",
    "\n",
    "trainwinloc1 = np.concatenate((trainwinloch1,trainwinlocf21,trainwinlocf31,trainwinlocc1))#,trainweightqntf41))\n",
    "trainwinloc2 = np.concatenate((trainwinloch2,trainwinlocf22,trainwinlocf32,trainwinlocc2))#,trainweightqntf42))\n",
    "trainwinloc3 = np.concatenate((trainwinloch3,trainwinlocf23,trainwinlocf33,trainwinlocc3))#,trainweightqntf43))\n",
    "trainwinloc0 = np.concatenate((trainwinloch0,trainwinlocf20,trainwinlocf30,trainwinlocc0))#,trainweightqntf41))\n",
    "trainwinloc4 = np.concatenate((trainwinloch4,trainwinlocf24,trainwinlocf34,trainwinlocc4))#,trainweightqntf41))\n",
    "\n",
    "trainpowerspec1 = np.concatenate((trainpowerspec_h1,trainpowerspec_f21,trainpowerspec_f31,trainpowerspec_c1))#,trainweightrff41))\n",
    "trainpowerspec2 = np.concatenate((trainpowerspec_h2,trainpowerspec_f22,trainpowerspec_f32,trainpowerspec_c2))#,trainweightrff42))\n",
    "trainpowerspec3 = np.concatenate((trainpowerspec_h3,trainpowerspec_f23,trainpowerspec_f33,trainpowerspec_c3))#,trainweightrff43))\n",
    "trainpowerspec0 = np.concatenate((trainpowerspec_h0,trainpowerspec_f20,trainpowerspec_f30,trainpowerspec_c0))#,trainweightrff41))\n",
    "trainpowerspec4 = np.concatenate((trainpowerspec_h4,trainpowerspec_f24,trainpowerspec_f34,trainpowerspec_c4))#,trainweightrff41))\n",
    "\n",
    "\n",
    "trainsimstats1 = np.concatenate((trainsimstats_h1,trainsimstats_f21,trainsimstats_f31,trainsimstats_c1))#,trainweightqntf41))\n",
    "trainsimstats2 = np.concatenate((trainsimstats_h2,trainsimstats_f22,trainsimstats_f32,trainsimstats_c2))#,trainweightqntf42))\n",
    "trainsimstats3 = np.concatenate((trainsimstats_h3,trainsimstats_f23,trainsimstats_f33,trainsimstats_c3))#,trainweightqntf43))\n",
    "trainsimstats0 = np.concatenate((trainsimstats_h0,trainsimstats_f20,trainsimstats_f30,trainsimstats_c0))#,trainweightqntf41))\n",
    "trainsimstats4 = np.concatenate((trainsimstats_h4,trainsimstats_f24,trainsimstats_f34,trainsimstats_c4))#,trainweightqntf41))\n",
    "\n",
    "trainstatloc1 = np.concatenate((trainstatloc_h1,trainstatloc_f21,trainstatloc_f31,trainstatloc_c1))#,trainweightqntf41))\n",
    "trainstatloc2 = np.concatenate((trainstatloc_h2,trainstatloc_f22,trainstatloc_f32,trainstatloc_c2))#,trainweightqntf42))\n",
    "trainstatloc3 = np.concatenate((trainstatloc_h3,trainstatloc_f23,trainstatloc_f33,trainstatloc_c3))#,trainweightqntf43))\n",
    "trainstatloc0 = np.concatenate((trainstatloc_h0,trainstatloc_f20,trainstatloc_f30,trainstatloc_c0))#,trainweightqntf41))\n",
    "trainstatloc4 = np.concatenate((trainstatloc_h4,trainstatloc_f24,trainstatloc_f34,trainstatloc_c4))#,trainweightqntf41))\n",
    "\n",
    "trainBmode1 = np.concatenate((trainBmodeh1,trainBmodef21,trainBmodef31,trainBmodec1))#,trainweightqntf41))\n",
    "trainBmode2 = np.concatenate((trainBmodeh2,trainBmodef22,trainBmodef32,trainBmodec2))#,trainweightqntf42))\n",
    "trainBmode3 = np.concatenate((trainBmodeh3,trainBmodef23,trainBmodef33,trainBmodec3))#,trainweightqntf43))\n",
    "trainBmode0 = np.concatenate((trainBmodeh0,trainBmodef20,trainBmodef30,trainBmodec0))#,trainweightqntf41))\n",
    "trainBmode4 = np.concatenate((trainBmodeh4,trainBmodef24,trainBmodef34,trainBmodec4))#,trainweightqntf41))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f55736abc880>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#run this one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/jupyter/AdisStuff/kfold_new38/trainwinloc0_kfold'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mtrainwinloc0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/jupyter/AdisStuff/kfold_new38/trainwinloc2_kfold'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mtrainwinloc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/jupyter/AdisStuff/kfold_new38/trainwinloc3_kfold'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mtrainwinloc3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/home/jupyter/AdisStuff/kfold_new38/trainwinloc1_kfold'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mtrainwinloc1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#run this one\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainwinloc0_kfold' ,trainwinloc0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainwinloc2_kfold' ,trainwinloc2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainwinloc3_kfold' ,trainwinloc3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainwinloc1_kfold' ,trainwinloc1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainwinloc4_kfold' ,trainwinloc4)\n",
    "\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainlabframe0_kfold' ,trainlabframe0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainframe0_kfold' ,trainframe0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainwind0_kfold' ,trainwind0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainlabvid0_kfold' ,trainlabvid0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainvid0_kfold' ,trainvid0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainframetovid0_kfold' ,trainframetovid0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainvidtopat0_kfold' ,trainvidtopat0)\n",
    "\n",
    "\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainlabframe1_kfold' ,trainlabframe1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainframe1_kfold' ,trainframe1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainwind1_kfold' ,trainwind1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainlabvid1_kfold' ,trainlabvid1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainvid1_kfold' ,trainvid1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainframetovid1_kfold' ,trainframetovid1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainvidtopat1_kfold' ,trainvidtopat1)\n",
    "\n",
    "\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainlabframe2_kfold' ,trainlabframe2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainframe2_kfold' ,trainframe2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainwind2_kfold' ,trainwind2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainlabvid2_kfold' ,trainlabvid2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainvid2_kfold' ,trainvid2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainframetovid2_kfold' ,trainframetovid2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainvidtopat2_kfold' ,trainvidtopat2)\n",
    "\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainlabframe3_kfold' ,trainlabframe3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainframe3_kfold' ,trainframe3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainwind3_kfold' ,trainwind3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainlabvid3_kfold' ,trainlabvid3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainvid3_kfold' ,trainvid3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainframetovid3_kfold' ,trainframetovid3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainvidtopat3_kfold' ,trainvidtopat3)\n",
    "\n",
    "\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainlabframe4_kfold' ,trainlabframe4)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainframe4_kfold' ,trainframe4)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainwind4_kfold' ,trainwind4)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainlabvid4_kfold' ,trainlabvid4)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainvid4_kfold' ,trainvid4)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainframetovid4_kfold' ,trainframetovid4)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainvidtopat4_kfold' ,trainvidtopat4)\n",
    "\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainweightrf0_kfold' ,trainweightrf0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainweightrf2_kfold' ,trainweightrf2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainweightrf3_kfold' ,trainweightrf3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainweightrf1_kfold' ,trainweightrf1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainweightrf4_kfold' ,trainweightrf4)\n",
    "\n",
    "\n",
    "\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainweightqnt0_kfold' ,trainweightqnt0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainweightqnt2_kfold' ,trainweightqnt2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainweightqnt3_kfold' ,trainweightqnt3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainweightqnt1_kfold' ,trainweightqnt1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/trainweightqnt4_kfold' ,trainweightqnt4)\n",
    "\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/windtoframe0_kfold' ,windtoframe0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/windtoframe2_kfold' ,windtoframe2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/windtoframe3_kfold' ,windtoframe3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/windtoframe1_kfold' ,windtoframe1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/windtoframe4_kfold' ,windtoframe4)\n",
    "\n",
    "\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainpowerspec0_kfold' ,trainpowerspec0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainpowerspec2_kfold' ,trainpowerspec2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainpowerspec3_kfold' ,trainpowerspec3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainpowerspec1_kfold' ,trainpowerspec1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainpowerspec4_kfold' ,trainpowerspec4)\n",
    "\n",
    "\n",
    "\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainsimstats0_kfold' ,trainsimstats0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainsimstats2_kfold' ,trainsimstats2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainsimstats3_kfold' ,trainsimstats3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainsimstats1_kfold' ,trainsimstats1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainsimstats4_kfold' ,trainsimstats4)\n",
    "\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainstatloc0_kfold' ,trainstatloc0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainstatloc2_kfold' ,trainstatloc2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainstatloc3_kfold' ,trainstatloc3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainstatloc1_kfold' ,trainstatloc1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainstatloc4_kfold' ,trainstatloc4)\n",
    "\n",
    "\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainBmode0_kfold' ,trainBmode0)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainBmode2_kfold' ,trainBmode2)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainBmode3_kfold' ,trainBmode3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainBmode1_kfold' ,trainBmode1)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38_L/trainBmode4_kfold' ,trainBmode4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/jupyter/AdisStuff/kfold_new38/c3k' ,c3k)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/f3k' ,f3k)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/f2k' ,f2k)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/h3k' ,h3k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/jupyter/AdisStuff/kfold_new38/c3' ,c3)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/f33' ,f33)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/f23' ,f23)\n",
    "np.save('/home/jupyter/AdisStuff/kfold_new38/h3' ,h3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(c3k[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qmap=np.zeros([1152,17])#\n",
    "Qmap[(Qmap==0).nonzero()]=np.nan\n",
    "Qmap[trainwinloc0[0],:]=trainwind0[0]\n",
    "Qmap=np.reshape(Qmap,(24,48,17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
